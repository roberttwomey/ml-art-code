{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roberttwomey/ml-art-code/blob/master/biggan/BigGAN_handson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLOYL1PJAAtK"
      },
      "source": [
        "# BigGAN Hands-On: Generating Images with BigGAN\n",
        "\n",
        "<!-- __NOTE for OOD users__: select the `tf-gpu-cyclegan` kernel. -->\n",
        "\n",
        "This notebook walks you through some of the basics of generating images with BigGAN. You can read more about BigGAN in [the paper on arXiv](https://arxiv.org/abs/1809.11096) [1]. It was adapted for ML for the Arts by rtwomey@unl.edu from [an example](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb) and [pytorch-pretrained-BigGAN](https://github.com/huggingface/pytorch-pretrained-BigGAN).\n",
        "\n",
        "We will move through this notebook and run each cell by pressing the **Play** button to the left of the cell.\n",
        "\n",
        "## Activities:\n",
        "\n",
        "1. BigGAN set up\n",
        "2. Generate images\n",
        "3. Explore latent vector math\n",
        "4. Generate interpolations\n",
        "5. Discussions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS1_N6hKj8cz"
      },
      "source": [
        "## Select a Model (optional)\n",
        "\n",
        "By default, this notebook will use the 256 x 256 pixel BigGAN-deep model to generate images (https://tfhub.dev/deepmind/biggan-deep-256/). To get started, I'd suggest leaving this unchanged.\n",
        "\n",
        "To generate 128x128 or 512x512 images or to use the original BigGAN generators, comment out the active **`module_path`** below and uncomment one of the others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJCIhQPClKJ1"
      },
      "outputs": [],
      "source": [
        "# BigGAN-deep models\n",
        "module_path = 'biggan-deep-128'\n",
        "# module_path = 'biggan-deep-256'\n",
        "# module_path = 'biggan-deep-512'\n",
        "\n",
        "\n",
        "# module_path = 'https://tfhub.dev/deepmind/biggan-deep-128/1'  # 128x128 BigGAN-deep\n",
        "# module_path = \"https://tfhub.dev/deepmind/biggan-deep-256/1\"  # 256x256 BigGAN-deep\n",
        "# module_path = 'https://tfhub.dev/deepmind/biggan-deep-512/1'  # 512x512 BigGAN-deep\n",
        "\n",
        "# BigGAN (original) models\n",
        "# module_path = 'https://tfhub.dev/deepmind/biggan-128/2'  # 128x128 BigGAN\n",
        "# module_path = 'https://tfhub.dev/deepmind/biggan-256/2'  # 256x256 BigGAN\n",
        "# module_path = 'https://tfhub.dev/deepmind/biggan-512/2'  # 512x512 BigGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJrTM6hAi0CJ"
      },
      "source": [
        "# 0. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install pytorch-pretrained-BIGGAN\n",
        "We will use this pretrained BigGAN implemented in pytorch: [pytorch-pretrained-BigGAN](https://github.com/huggingface/pytorch-pretrained-BigGAN)"
      ],
      "metadata": {
        "id": "Eqi1-mDaaVu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-biggan"
      ],
      "metadata": {
        "id": "KgFQd9GDU6di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install other libraries"
      ],
      "metadata": {
        "id": "rp1h134aavNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display\n",
        "import numpy as np\n",
        "import io\n",
        "import PIL.Image\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "bqSAffpVVSE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry62-8SWfuds"
      },
      "source": [
        "## Define some functions for sampling and displaying BigGAN images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def imgrid(imarray, cols=5, pad=1):\n",
        "  if imarray.dtype != np.uint8:\n",
        "    raise ValueError('imgrid input imarray must be uint8')\n",
        "  pad = int(pad)\n",
        "  assert pad >= 0\n",
        "  cols = int(cols)\n",
        "  assert cols >= 1\n",
        "  N, H, W, C = imarray.shape\n",
        "  rows = N // cols + int(N % cols != 0)\n",
        "  batch_pad = rows * cols - N\n",
        "  assert batch_pad >= 0\n",
        "  post_pad = [batch_pad, pad, pad, 0]\n",
        "  pad_arg = [[0, p] for p in post_pad]\n",
        "  imarray = np.pad(imarray, pad_arg, 'constant', constant_values=255)\n",
        "  H += pad\n",
        "  W += pad\n",
        "  grid = (imarray\n",
        "          .reshape(rows, cols, H, W, C)\n",
        "          .transpose(0, 2, 1, 3, 4)\n",
        "          .reshape(rows*H, cols*W, C))\n",
        "  if pad:\n",
        "    grid = grid[:-pad, :-pad]\n",
        "  return grid\n",
        "\n",
        "def imshow(a, format='png', jpeg_fallback=True):\n",
        "  a = np.asarray(a, dtype=np.uint8)\n",
        "  data = io.BytesIO()\n",
        "  PIL.Image.fromarray(a).save(data, format)\n",
        "  im_data = data.getvalue()\n",
        "  try:\n",
        "    disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  except IOError:\n",
        "    if jpeg_fallback and format != 'jpeg':\n",
        "      print(('Warning: image was too large to display in format \"{}\"; '\n",
        "             'trying jpeg instead.').format(format))\n",
        "      return imshow(a, format='jpeg')\n",
        "    else:\n",
        "      raise\n",
        "  return disp\n",
        "\n",
        "def sample(model, z, y, truncation):\n",
        "  # convert to torch\n",
        "  noise_vector = torch.from_numpy(z)\n",
        "  class_vector = torch.from_numpy(y)\n",
        "\n",
        "  # If you have a GPU, put everything on cuda\n",
        "  noise_vector = noise_vector.to('cuda')\n",
        "  class_vector = class_vector.to('cuda')\n",
        "  model.to('cuda')\n",
        "\n",
        "  # run the generator\n",
        "  with torch.no_grad():\n",
        "    output = model(noise_vector, class_vector, truncation)\n",
        "\n",
        "  # If you have a GPU put back on CPU\n",
        "  output = output.to('cpu')\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "f5b1WMb2ZZBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stWb21nlcyCm"
      },
      "source": [
        "## Load a pretrained BigGAN generator\n",
        "\n",
        "(takes about 1 minute to run)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, one_hot_from_int,\n",
        "                                       truncated_noise_sample, save_as_images,\n",
        "                                       display_in_terminal, convert_to_images)\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load pre-trained model\n",
        "model = BigGAN.from_pretrained(module_path)"
      ],
      "metadata": {
        "id": "MTzXEIyrU5DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeZ7u3rWd9jz"
      },
      "source": [
        "# 1. Explore BigGAN samples of a particular category\n",
        "\n",
        "Let's generate an image with BigGAN!\n",
        "\n",
        "We are going to specify two things:\n",
        "- a `y` vector (__class vector__ 1 x 1000 long). This sets which _kind_ of image BigGAN will generate.\n",
        "  - by default `y` is a \"one hot\" vector, which means means it is all zero, except for a `1` for the type of object it is.\n",
        "- a `z` vector (__noise vector__ 1 x 128 long). this sets _which_ particular instance it generates.\n",
        "\n",
        "We can change `y`, `z`, and a few other variables to change the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95A8X_QWKbo2"
      },
      "source": [
        "Seed our randoms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noise_seed = 0"
      ],
      "metadata": {
        "id": "28s6Z_Zsdwm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate images for your category"
      ],
      "metadata": {
        "id": "8jijCRsspnJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare an input to biggan\n",
        "truncation = 0.5\n",
        "num_samples = 10\n",
        "\n",
        "class_vector = one_hot_from_names(['cheeseburger'], batch_size=num_samples)\n",
        "# class_vector = one_hot_from_int([933], batch_size=num_samples)\n",
        "noise_vector = truncated_noise_sample(truncation=truncation, batch_size=num_samples, seed=noise_seed)\n",
        "\n",
        "# convert to torch\n",
        "noise_vector = torch.from_numpy(noise_vector)\n",
        "class_vector = torch.from_numpy(class_vector)\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "noise_vector = noise_vector.to('cuda')\n",
        "class_vector = class_vector.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# run the generator\n",
        "with torch.no_grad():\n",
        "  output = model(noise_vector, class_vector, truncation)\n",
        "\n",
        "# If you have a GPU put back on CPU\n",
        "output = output.to('cpu')"
      ],
      "metadata": {
        "id": "AoI7TiWpbZ3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "view a result:"
      ],
      "metadata": {
        "id": "wMg7-DFnbFOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = convert_to_images(output)\n",
        "results[2]"
      ],
      "metadata": {
        "id": "3YaKhHX_YKyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "show as a grid using our helper functions"
      ],
      "metadata": {
        "id": "3ZhMJvqbcn6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imshow(imgrid(np.array(results)))"
      ],
      "metadata": {
        "id": "0zUHhNRpb5Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: save outputs as pngs (uncomment the following)"
      ],
      "metadata": {
        "id": "RhX_Gfvta7Sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save_as_images(output)"
      ],
      "metadata": {
        "id": "t8QiEw0da5Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try another category"
      ],
      "metadata": {
        "id": "RK_WUqc7phcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok. Now try some other categories.\n",
        "\n",
        "You can see the full list of categories here in this list: [imagenet1000 class ids](https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt).\n",
        "\n",
        "Just type in the number (f.ex. `933` for cheeseburger) for the variable `category` below."
      ],
      "metadata": {
        "id": "YUyGQWddfOM_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuCO9tv3IKT2"
      },
      "outputs": [],
      "source": [
        "# set parameters\n",
        "num_samples = 1  # between 1 and 20\n",
        "noise_seed = 0  # using the same number will give you the same results, repeatedly random!\n",
        "truncation = 0.5  # between 0.02 and 1\n",
        "\n",
        "# set category\n",
        "category = 800 # slot machine"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create our input vectors and generate our image. We are going to use our\n",
        "`sample()` function to make it easier to generate images from the BigGAN model."
      ],
      "metadata": {
        "id": "gDL2U8A7rrg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_vector = one_hot_from_int([category], batch_size=num_samples)\n",
        "noise_vector = truncated_noise_sample(truncation=truncation, batch_size=num_samples, seed=noise_seed)\n",
        "\n",
        "# generate the output from the model\n",
        "output = sample(model, noise_vector, class_vector, truncation)"
      ],
      "metadata": {
        "id": "RWYivtCjeRZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = convert_to_images(output)\n",
        "if (num_samples > 1):\n",
        "  imshow(imgrid(np.array(results)))\n",
        "else:\n",
        "  imshow(results[0])"
      ],
      "metadata": {
        "id": "XRq0QJgpenkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dwZJp26Kbo3"
      },
      "outputs": [],
      "source": [
        "print(noise_vector) # lets see what our noise vector was"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT6uFvrXKbo3"
      },
      "outputs": [],
      "source": [
        "print(class_vector) # lets see what our class vector was. See how it is all zero except for one `1`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "notice that `class_vector` array is all equal to `0` except for one `1`. that is what makes it a **one hot** array."
      ],
      "metadata": {
        "id": "PagwWV3OopIc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nihy-FAxKbo3"
      },
      "source": [
        "## Activities:\n",
        "- try exploring different __`noise_seeds`__. When you put in a different number for this, how does the result change?\n",
        "- try changing the **`truncation`** value (between 0.02 and 1.0), how does this change the result?\n",
        "- change the `category`. You can see the full list of categories here in this list: [imagenet1000 class ids](https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt). Just type in the number (f.ex. `933` for cheeseburger)\n",
        "  - (these are 1000 categories from the larger ImageNet dataset which BigGAN was trained on)\n",
        "  - [Nice youtube video](https://youtu.be/YY6LrQSxIbc) of the 1000 classes from BigGAN\n",
        "- you can generate a set of outputs. f.ex. increase `num_samples` to `10`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN8GlffTKbo3"
      },
      "source": [
        "# 2. Class Arithmetic (combinations of 2 or more categories)\n",
        "\n",
        "If we have one vector for object 1, and one vector for object 2, we can make combinations of objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUsrFlOCKbo3"
      },
      "outputs": [],
      "source": [
        "# sampling parameters\n",
        "truncation = 0.2\n",
        "\n",
        "# class and noise vectors\n",
        "noise_seed_A = 0\n",
        "category_A = 207 # golden retriever\n",
        "# category_A = 850 # teddy bear\n",
        "\n",
        "noise_seed_B = 0\n",
        "category_B = 8 # hen\n",
        "# category_B = 872 # tripod\n",
        "\n",
        "# noise and class for image 1 (golden retriever)\n",
        "z_A = truncated_noise_sample(truncation=truncation, batch_size=1, seed=noise_seed_A)\n",
        "y_A = one_hot_from_int([category_A], batch_size=1)\n",
        "\n",
        "# noise and class for image 2 (hen)\n",
        "z_B = truncated_noise_sample(truncation=truncation, batch_size=1, seed=noise_seed_A)\n",
        "y_B = one_hot_from_int([category_B], batch_size=1)\n",
        "\n",
        "# make a combination\n",
        "percent_A = 0.7 # how much of dog\n",
        "percent_B = 1.0 - percent_A  # how much of hen\n",
        "\n",
        "z_combo = percent_A * z_A + percent_B * z_B\n",
        "y_combo = percent_A * y_A + percent_B * y_B\n",
        "\n",
        "# Generate the images\n",
        "output = sample(model, z_combo, y_combo, truncation)\n",
        "\n",
        "# show the result\n",
        "results = convert_to_images(output)\n",
        "results[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUqRWivpKbo3"
      },
      "source": [
        "## Activities\n",
        "- try changing the `percent_A` to make the hybrid more dog, or more hen\n",
        "- try your own two classes from the list above\n",
        "- try three (!).\n",
        "  - (you can do it: it's just algebra)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHNXtvuQgKwa"
      },
      "source": [
        "# 3. Interpolate between BigGAN samples\n",
        "\n",
        "Now we are going calculate multiple intermediate samples between two different images.\n",
        "\n",
        "First lets make helper functions to interpolate between vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uKeRK6QKbo4"
      },
      "outputs": [],
      "source": [
        "def interpolate(A, B, num_interps):\n",
        "  if A.shape != B.shape:\n",
        "    raise ValueError('A and B must have the same shape to interpolate.')\n",
        "  alphas = np.linspace(0, 1, num_interps)\n",
        "  return np.array([(1-a)*A + a*B for a in alphas])\n",
        "\n",
        "def interpolate_and_shape(A, B, num_samples, num_interps):\n",
        "  interps = interpolate(A, B, num_interps)\n",
        "  return (interps.transpose(1, 0, *range(2, len(interps.shape)))\n",
        "                 .reshape(num_samples * num_interps, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try two different classes (`category_A`, and `category_B`). We are going to use the interpolate functions to generate intermediate steps between those things.\n",
        "\n",
        "(Try 48 for komodo dragon, and 8 for hen)"
      ],
      "metadata": {
        "id": "OR8UcZ30qUGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSAyfDfnVugs"
      },
      "outputs": [],
      "source": [
        "# sampling parameters\n",
        "num_samples = 2\n",
        "num_interps = 10\n",
        "truncation = 0.25\n",
        "\n",
        "# class and noise vectors\n",
        "noise_seed_A = 0\n",
        "category_A = 48 # komodo dragon\n",
        "# category_A = 850 # teddy bear\n",
        "\n",
        "noise_seed_B = 0\n",
        "category_B = 8 # hen\n",
        "# category_B = 872 # tripod\n",
        "\n",
        "# noise vectors\n",
        "z_A, z_B = [\n",
        "    truncated_noise_sample(truncation=truncation, batch_size=num_samples, seed=noise_seed)\n",
        "    for noise_seed in [noise_seed_A, noise_seed_B]\n",
        "]\n",
        "\n",
        "# class vectors\n",
        "y_A, y_B = [\n",
        "    one_hot_from_int([category] , batch_size=num_samples)\n",
        "    for category in [category_A, category_B]\n",
        "]\n",
        "\n",
        "z_interp = interpolate_and_shape(z_A, z_B, num_samples, num_interps)\n",
        "y_interp = interpolate_and_shape(y_A, y_B, num_samples, num_interps)\n",
        "\n",
        "output = sample(model, z_interp, y_interp, truncation=truncation)\n",
        "ims = convert_to_images(output)\n",
        "imshow(imgrid(np.array(ims), cols=num_interps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24bQgh9wKbo4"
      },
      "outputs": [],
      "source": [
        "imshow(ims[2]) # show one hybrid in-between state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J9u8K1gKbo4"
      },
      "outputs": [],
      "source": [
        "# save one image to disk\n",
        "save_as_images(output[2].unsqueeze(0), \"my_hybrid.jpg\")\n",
        "\n",
        "# save all the images\n",
        "# save_as_images(output, \"hybrids.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OCuxIIzAAvl"
      },
      "source": [
        "## Activities\n",
        "\n",
        "- Try varying the `truncation`\n",
        "- Try varying the `noise_seed_A` and `noise_seed_B`\n",
        "- Pick your own categories for `category_A` and `category_B`.\n",
        "  - What is an ideal hybrid image to make?\n",
        "  - reminder: you can see the full list of categories here in this list: [imagenet1000 class ids](https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt). Just type in the number (f.ex. `933` for cheeseburger)\n",
        "- Increase `num_interps` to increase the number of intermediate steps between the two classes.\n",
        "- Try to combine 3+ classes with arithmetic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fuS0ckNKbo5"
      },
      "source": [
        "# 3a. Interpolation with Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgmZHEP8Kbo5"
      },
      "source": [
        "helper functions for video interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYw0UCgbKbo5"
      },
      "outputs": [],
      "source": [
        "def interpolate_and_shape(A, B, num_samples, num_interps):\n",
        "  interps = interpolate(A, B, num_interps)\n",
        "  return (interps.transpose(1, 0, *range(2, len(interps.shape)))\n",
        "                 .reshape(num_samples * num_interps, -1))\n",
        "\n",
        "def get_interpolated_yz(categories_all, num_interps, noise_seed_A, noise_seed_B, truncation):\n",
        "  nt = len(categories_all)\n",
        "  num_samples = 1\n",
        "  z_A, z_B = [truncated_noise_sample(truncation=truncation, batch_size=num_samples, seed=noise_seed)\n",
        "              for noise_seed in [noise_seed_A, noise_seed_B]]\n",
        "\n",
        "  y_interps = []\n",
        "  for i in range(nt):\n",
        "    category_A, category_B = categories_all[i], categories_all[(i+1)%nt]\n",
        "    y_A, y_B = [one_hot_from_int([category], batch_size=num_samples) for category in [category_A, category_B]]\n",
        "\n",
        "\n",
        "    y_interp = interpolate_and_shape(np.array(y_A), np.array(y_B), num_samples, num_interps)\n",
        "    y_interps.append(y_interp)\n",
        "\n",
        "  y_interp = np.vstack(y_interps)\n",
        "  z_interp = interpolate_and_shape(z_A, z_B, num_samples, num_interps * nt)\n",
        "\n",
        "  return y_interp, z_interp\n",
        "\n",
        "def get_transition_yz(classes, num_interps, truncation):\n",
        "  noise_seed_A, noise_seed_B = 10, 20   # fix this!\n",
        "  return get_interpolated_yz(classes, num_interps, noise_seed_A, noise_seed_B, truncation=truncation)\n",
        "\n",
        "def get_random_yz(num_classes, num_interps, truncation):\n",
        "  random_classes = [ int(1000*random()) for i in range(num_classes) ]\n",
        "  return get_transition_yz(random_classes, num_interps, truncation=truncation)\n",
        "\n",
        "def get_combination_yz(categories, noise_seed, truncation):\n",
        "  z = np.vstack([truncated_z_sample(1, truncation, noise_seed)] * (len(categories)+1))\n",
        "  y = np.zeros((len(categories)+1, 1000))\n",
        "  for i, c in enumerate(categories):\n",
        "    y[i, c] = 1.0\n",
        "    y[len(categories), c] = 1.0\n",
        "  return y, z\n",
        "\n",
        "def slerp(A, B, num_interps):  # see https://en.wikipedia.org/wiki/Slerp\n",
        "  alphas = np.linspace(-1.5, 2.5, num_interps) # each unit step tends to be a 90 degree rotation in high-D space, so this is ~360 degrees\n",
        "  omega = np.zeros((A.shape[0],1))\n",
        "  for i in range(A.shape[0]):\n",
        "      tmp = np.dot(A[i],B[i])/(np.linalg.norm(A[i])*np.linalg.norm(B[i]))\n",
        "      omega[i] = np.arccos(np.clip(tmp,0.0,1.0))+1e-9\n",
        "  return np.array([(np.sin((1-a)*omega)/np.sin(omega))*A + (np.sin(a*omega)/np.sin(omega))*B for a in alphas])\n",
        "\n",
        "def slerp_and_shape(A, B, num_interps):\n",
        "  interps = slerp(A, B, num_interps)\n",
        "  return (interps.transpose(1, 0, *range(2, len(interps.shape)))\n",
        "                 .reshape(num_interps, *interps.shape[2:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Zn7f7HKbo5"
      },
      "source": [
        "Video helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNYd2oOHKbo5"
      },
      "outputs": [],
      "source": [
        "def save_images(imgs, filepath):\n",
        "  for i, img in enumerate(imgs):\n",
        "    outfile = os.path.join(filepath,\"output_%05d.jpg\"%i)\n",
        "    PIL.Image.fromarray(img).save(outfile)\n",
        "\n",
        "def save_images_with_hold(imgs, filepath, num_interps, len_hold):\n",
        "  count = 0\n",
        "  for i, img in enumerate(imgs):\n",
        "    outfile = os.path.join(filepath,\"output_%05d\"%count)\n",
        "    # thisout = PIL.Image.fromarray(img)\n",
        "    # thisout.save(outfile)\n",
        "    save_as_images(img.unsqueeze(0), outfile)\n",
        "    count+=1\n",
        "\n",
        "    if i%num_interps == 0:\n",
        "        for j in range(len_hold):\n",
        "            outfile = os.path.join(filepath,\"output_%05d\"%count)\n",
        "            # thisout.save(outfile)\n",
        "            save_as_images(img.unsqueeze(0), outfile)\n",
        "            count+=1\n",
        "  return count\n",
        "\n",
        "# REMOVED OPENCV VIDEO CREATION FOR NOW\n",
        "\n",
        "# def make_video(video_name, imgs):\n",
        "#   _, height, width, _ = imgs.shape\n",
        "#   video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc('M','J','P','G'), fps=24, frameSize=(width,height))\n",
        "#   for iter in range(0,imgs.shape[0]):\n",
        "#       video.write(imgs[iter,:,:,::-1])\n",
        "#   cv2.destroyAllWindows()\n",
        "#   video.release()\n",
        "# #   files.download(video_name)\n",
        "#   print(\"download \", video_name)\n",
        "\n",
        "\n",
        "# def make_video_from_samples(video_name, sess, noise, label, truncation=1.0, batch_size=8, vocab_size=vocab_size):\n",
        "#   height, width = 512, 512\n",
        "#   video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc('M','J','P','G'), fps=30, frameSize=(width,height))\n",
        "#   noise = np.asarray(noise)\n",
        "#   label = np.asarray(label)\n",
        "#   num = noise.shape[0]\n",
        "#   if len(label.shape) == 0:\n",
        "#     label = np.asarray([label] * num)\n",
        "#   if label.shape[0] != num:\n",
        "#     raise ValueError('Got # noise samples ({}) != # label samples ({})'\n",
        "#                      .format(noise.shape[0], label.shape[0]))\n",
        "#   label = one_hot_if_needed(label, vocab_size)\n",
        "#   ims = []\n",
        "#   for batch_start in tqdm(xrange(0, num, batch_size)):\n",
        "#     s = slice(batch_start, min(num, batch_start + batch_size))\n",
        "#     feed_dict = {input_z: noise[s], input_y: label[s], input_trunc: truncation}\n",
        "#     ims = [sess.run(output, feed_dict=feed_dict)]\n",
        "#     ims = np.concatenate(ims, axis=0)\n",
        "#     ims = np.clip(((ims + 1) / 2.0) * 256, 0, 255)\n",
        "#     ims = np.uint8(ims)\n",
        "#     for iter in range(0,ims.shape[0]):\n",
        "#       video.write(ims[iter,:,:,::-1])\n",
        "#   cv2.destroyAllWindows()\n",
        "#   video.release()\n",
        "# #   files.download(video_name)\n",
        "#   print(\"download \", video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHiJTA6ZKbo5"
      },
      "source": [
        "file paths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op9qinCMKbo5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# file paths\n",
        "work = os.getcwd() # get the current path\n",
        "\n",
        "# the file directories\n",
        "# workbase = %env WORK\n",
        "workbase = \"/content\"\n",
        "\n",
        "# results\n",
        "resultsbase = os.path.join(workbase, \"biggan\", \"results/\")\n",
        "\n",
        "# intermediate frames working directory\n",
        "interpbase = os.path.join(workbase, \"biggan\", \"interpolation/\")\n",
        "\n",
        "# video output files\n",
        "filebase = 'myinterp_%d_%d'\n",
        "moviefilename = filebase+'.mp4'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSvai21zKbo5"
      },
      "source": [
        "delete old results and make new folder if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsD1U-KbKbo5"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $interpbase\n",
        "!rm $interpbase/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODhQWnIxKbo5"
      },
      "outputs": [],
      "source": [
        "# frame rate for the the movie\n",
        "fps = 30\n",
        "len_hold = 30 # how many frames to pause on each sample\n",
        "num_interps = 180 # how many frames to transition between the sample\n",
        "\n",
        "# parameters for sampling from model\n",
        "num_samples = 2\n",
        "truncation = 0.25\n",
        "\n",
        "# class and noise vectors\n",
        "noise_seed_A = 0\n",
        "category_A = 48\n",
        "\n",
        "noise_seed_B = 0\n",
        "category_B = 8 # hen\n",
        "\n",
        "\n",
        "y_interp, z_interp = get_interpolated_yz([category_A, category_B], num_interps, noise_seed_A, noise_seed_B, truncation=truncation)\n",
        "imgs = sample(model, z_interp, y_interp, truncation=truncation)\n",
        "\n",
        "# save_images(imgs, interpbase)\n",
        "# count = num_interps\n",
        "count = save_images_with_hold(imgs, interpbase, num_interps, len_hold)\n",
        "print(\"saved {0} images out... to {1}\".format(count, interpbase))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7jvyJcXKbo6"
      },
      "outputs": [],
      "source": [
        "fps = 30\n",
        "\n",
        "out = moviefilename%(category_A, category_B)\n",
        "# with open('list.txt','w') as f:\n",
        "#   for i in range(count*2):\n",
        "#     f.write('file %s/output_%05d.jpg\\n'%(interpbase, i))\n",
        "cmd = \"ffmpeg -r {0} -i {1}/output_%05d_0.png -c:v libx265 -pix_fmt yuv420p -crf 0 -r {0} {2} -y\"\n",
        "\n",
        "os.system(cmd.format(fps, interpbase, out))\n",
        "print(cmd.format(fps, interpbase, out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVE01gl-Kbo6"
      },
      "source": [
        "### To make the movie file\n",
        "1. open a terminal here in jupterhub.\n",
        "2. type `module load ffmpeg` to load the ffmpeg program.\n",
        "3. cut and past the ffmpeg command above to generate your video file. This will create an output .mp4 file in your current directory.\n",
        "4. Download the video file from the file browser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHeV6BUBKbo6"
      },
      "source": [
        "### Activities\n",
        "- Try with your own classes for `category_A` and `category_B`\n",
        "- Try with 3 or more classes. Replace `[category_A, category_B]` in `get_interpolated_yz` with 3 or more numbers.\n",
        "  - f.ex.: `[48, 356, 7]` for komodo, ox, chicken.\n",
        "  - don't forget to change the output filename or save a local copy.\n",
        "- try changing the fps, the `num_interps` (which sets how many steps between each image), and `len_hold`, which sets how long we pause on each image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUMJj3zFKbo6"
      },
      "source": [
        "# 4. Opposite of a class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVB2E1QVB0cT"
      },
      "outputs": [],
      "source": [
        "# num_samples = 10\n",
        "# truncation = 0.4\n",
        "# noise_seed = 14\n",
        "# category = 603\n",
        "\n",
        "# z = truncated_z_sample(num_samples, truncation, noise_seed)\n",
        "# y = one_hot([category] * num_samples)\n",
        "\n",
        "# # print(y)\n",
        "\n",
        "# # invert y (opposite of horse cart)\n",
        "# # y = (1.0 - y)/(len(y[0]))\n",
        "# y = y * -1.0\n",
        "\n",
        "# print(y)\n",
        "\n",
        "# ims = sample(sess, z, y, truncation=truncation)\n",
        "# imshow(imgrid(ims, cols=min(num_samples, 5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWBuAno_Kbo7"
      },
      "source": [
        "# References\n",
        "\n",
        "[1] Andrew Brock, Jeff Donahue, and Karen Simonyan. [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096). *arxiv:1809.11096*, 2018.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BigGAN Hands-On",
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (tf-gpu-cyclegan)",
      "language": "python",
      "name": "tf-gpu-cyclegan"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}