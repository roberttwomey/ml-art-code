{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roberttwomey/ml-art-code/blob/master/clip-biggan/clip-biggan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OewBG-LxTHp4"
      },
      "source": [
        "EMAR349 ML for the Arts - Twomey - Spring 2024 - [ml.roberttwomey.com](http://ml.roberttwomey.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwTP4MYk0bYn"
      },
      "source": [
        "# Text to Image Synthesis (Storing Results)\n",
        "\n",
        "This notebook does text to image translation. It uses BigGAN to generate the images, and CLIP (a network from OpenAI that learns associations between images and text) to guide the image generation. It actually does an evolutionary search, so for each iteration it produces a population of candidate images, and then evaluates them towards a fitness criteria to proceed towards better translations.\n",
        "\n",
        "Based on [j.mp/wanderclip](https://j.mp/wanderclip) by Eyal Gruss [@eyaler](https://twitter.com/eyaler) [eyalgruss.com](https://eyalgruss.com)\n",
        "\n",
        "Modified to run on HCC OOD/nautilus.optiputer.net/z8 by robert.twomey@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wJu2_lVTHp6"
      },
      "source": [
        "# Activities\n",
        "\n",
        "Change the textual prompt below, then select Run->Run All Cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0wpwH8HTHp6"
      },
      "source": [
        "<!-- # Start Here, Then Run All\n",
        "\n",
        "Results show up in `/work/results/` in the file browser at left -->\n",
        "\n",
        "# 1. Set the Prompt\n",
        "\n",
        "This is the text that you want to translate into an image. You can do an array of prompts, with each line in quotes, separated by commas. Or just do a single prompt. Edit the line in red below to be the text that you want to translate into an image.\n",
        "\n",
        "I've left in a number of my old prompts [from this project](http://roberttwomey.com/2021/09/performance-at-pom21-berlin-beyond-classification/) as comments (green lines with a `#` at the beginning), for your reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EWmKTmvBg7z5"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "#     \"a euglena in a petri dish\",\n",
        "#     \"a ginger cat\"\n",
        "#     \"a viola provides the potential for communication\"\n",
        "#     \"the potentiality of the virtual space\"\n",
        "#     \"a human living with a machine\"\n",
        "#     \"decomposing matter with the roots of a plant\"\n",
        "#     \"a euglena seen through a microscope\"\n",
        "#     \"a unexpected surprise\"\n",
        "#     \"the decomposition of an artificial intelligence\"\n",
        "#     \"the extraction of thought from human writers\"\n",
        "#     \"setting our machines loose into rich nourishing nature\"\n",
        "    \"a cat in a refrigerator\"\n",
        "#     \"Paul Rudd eating mac and cheese\"\n",
        "#     \"a family of robotic seals\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK9T2qqjTHp6"
      },
      "source": [
        "# 2. Set the random seed and the iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-ZqKdv_THp6"
      },
      "outputs": [],
      "source": [
        "# seeding/iteration\n",
        "seed = 128#255#1#5#9#3\n",
        "iterations = 40 #100#40\n",
        "terminal_iterations = iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08_hMBheTHp6"
      },
      "source": [
        "Set the file paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJL-I7INTHp6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# work = %env WORK\n",
        "# work = work + \"/clip_biggan\"\n",
        "# work = \"/work/twomeylab/rtwomey/clip-biggan/\"\n",
        "\n",
        "# file paths\n",
        "work = os.getcwd() # get the current path\n",
        "outpath = os.path.join(work, \"outputs/\")\n",
        "storedpath = os.path.join(work, \"process/\")\n",
        "resultspath = os.path.join(work, \"results/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyizqNK4THp7"
      },
      "source": [
        "Make output directories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuulqYcVTHp7"
      },
      "outputs": [],
      "source": [
        "# directories are defined up top\n",
        "!rm -rf $outpath\n",
        "!mkdir -p $outpath\n",
        "!mkdir -p $storedpath\n",
        "!mkdir -p $resultspath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4He1W2mTHp7"
      },
      "source": [
        "turn on the timer so we can see how long things take"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJbdtCtDTHp7"
      },
      "outputs": [],
      "source": [
        "# %pip install autotime\n",
        "# %load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxBWrHdVTHp7"
      },
      "source": [
        "<!--\n",
        "# 0. Configuration\n",
        "(install packages, not needed on OOD anymore)\n",
        "# # for UNL CRANE - gpu_v100\n",
        "# %pip install ipython-autotime\n",
        "# %pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "# %pip install pytorch-pretrained-biggan\n",
        "# %pip install nltk\n",
        "# %pip install cma\n",
        "# %pip install scipy\n",
        "# %pip install ftfy\n",
        "# %pip install imageio -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUHyaEidTHp7"
      },
      "source": [
        "# 3. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xy9oM1BTHp7"
      },
      "source": [
        "CLIP was installed from Install CLIP from source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIEawpOQTHp7"
      },
      "outputs": [],
      "source": [
        "! pip install cma\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install pytorch_pretrained_biggan\n",
        "# !git clone https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6dl10h5THp7"
      },
      "source": [
        "## Load Models:\n",
        "\n",
        "- BigGAN Deep 512\n",
        "- CLIP OpenAI\n",
        "- Wordnet\n",
        "- CMA\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J_KEYww2i19o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1SDpkkK7cU1y"
      },
      "outputs": [],
      "source": [
        "from pytorch_pretrained_biggan import BigGAN\n",
        "last_gen_model = 'biggan-deep-512'\n",
        "biggan_model = BigGAN.from_pretrained(last_gen_model).cuda().eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HodZbb7uTHp7"
      },
      "outputs": [],
      "source": [
        "# %cd /content/CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev8IAMpbTHp7"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "last_clip_model = 'ViT-B/32'\n",
        "# clip_model = \"ViT-L/14\"\n",
        "perceptor, preprocess = clip.load(last_clip_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9DfpUQdTHp7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqoseqnNTHp7"
      },
      "outputs": [],
      "source": [
        "import cma\n",
        "from cma.sigma_adaptation import CMAAdaptSigmaCSA, CMAAdaptSigmaTPA\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIe2wc8gTHp7"
      },
      "source": [
        "# 4. General CLIP+BigGAN+CMA-ES Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBEfVsOLTHp7"
      },
      "source": [
        "We set our prompts and seed above. But these are general guidelines:\n",
        "\n",
        "1. For **prompt** OpenAI suggest to use the template \"A photo of a X.\" or \"A photo of a X, a type of Y.\" [[paper]](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)\n",
        "2. For **initial_class** you can either use free text or select a special option from the drop-down list.\n",
        "3. Free text and 'From prompt' might fail to find an appropriate ImageNet class.\n",
        "4. **seed**=0 means no seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOWzPLrBbdxW"
      },
      "outputs": [],
      "source": [
        "# prompt is set above\n",
        "gen_model = 'biggan-deep' #@param ['biggan-deep', 'sigmoid']\n",
        "size = '512' #@param [512, 256, 128]\n",
        "color = True #@param {type:'boolean'}\n",
        "initial_class = 'Random mix' #@param ['From prompt', 'Random class', 'Random Dirichlet', 'Random mix', 'Random embeddings'] {allow-input: true}\n",
        "optimize_class = True #@param {type:'boolean'}\n",
        "class_smoothing = 0.1 #@param {type:'number'}\n",
        "truncation = 1 #@param {type:'number'}\n",
        "stochastic_truncation = False #@param {type:'boolean'}\n",
        "optimizer = 'CMA-ES' #@param ['SGD','Adam','CMA-ES','CMA-ES + SGD interleaved','CMA-ES + Adam interleaved','CMA-ES + terminal SGD','CMA-ES + terminal Adam']\n",
        "pop_size = 50 #@param {type:'integer'}\n",
        "if \"clip_model\" not in globals():\n",
        "#     clip_model ='ViT-L/14'\n",
        "    clip_model = 'ViT-B/32' #@param ['ViT-B/32','RN50','RN101','RN50x4']\n",
        "else:\n",
        "    clip_model= last_clip_model\n",
        "augmentations =  64#@param {type:'integer'}\n",
        "learning_rate =  0.1#@param {type:'number'}\n",
        "noise_normality_loss =  0#@param {type:'number'}\n",
        "embed_normality_loss = 0 #@param {type:'number'}\n",
        "minimum_entropy_loss = 0.0001 #@param {type:'number'}\n",
        "total_variation_loss = 0.1 #@param {type:'number'}\n",
        "# iterations = 100 #@param {type:'integer'}\n",
        "# terminal_iterations = 100 #@param {type:'integer'}\n",
        "\n",
        "# # live POM\n",
        "# iterations = iterations #20#100#40\n",
        "# terminal_iterations = iterations\n",
        "# show_every = 1 #@param {type:'integer'}\n",
        "# save_every = 5 #@param {type:'integer'}\n",
        "# fps = 1 #@param {type:'number'}\n",
        "# freeze_secs = 0 #@param {type:'number'}\n",
        "\n",
        "# non-live POM\n",
        "# iterations = 50 # from above\n",
        "# terminal_iterations = iterations\n",
        "show_every = 1 #@param {type:'integer'}\n",
        "save_every = 5 #@param {type:'integer'}\n",
        "fps = 1 #@param {type:'number'}\n",
        "freeze_secs = 0 #@param {type:'number'}\n",
        "seed = 13#@param {type:'number'}\n",
        "\n",
        "# other settings, don't need to change these\n",
        "softmax_temp = 1\n",
        "emb_factor = 0.067 #calculated empirically\n",
        "loss_factor = 100\n",
        "sigma0 = 0.5 #http://cma.gforge.inria.fr/cmaes_sourcecode_page.html#practical\n",
        "cma_adapt = True\n",
        "cma_diag = 'sigmoid' in gen_model\n",
        "cma_active = True\n",
        "cma_elitist = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xlx0ZWmTHp8"
      },
      "source": [
        "# 3. Generate\n",
        "\n",
        "At this point you can click \"Run->Run Selected Cell and All Below\" if you want.\n",
        "\n",
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWlk_6TETHp8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import sys\n",
        "import imageio\n",
        "from IPython.display import HTML, Image, clear_output\n",
        "from scipy.stats import truncnorm, dirichlet\n",
        "from pytorch_pretrained_biggan import BigGAN, convert_to_images, one_hot_from_names, utils\n",
        "from nltk.corpus import wordnet as wn\n",
        "#from base64 import b64encode\n",
        "from time import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPIln6pXTHp8"
      },
      "source": [
        "Seeding (repeatable randomness):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb3M4FX7THp8"
      },
      "outputs": [],
      "source": [
        "if seed == 0:\n",
        "    seed = None\n",
        "    state = None\n",
        "else:\n",
        "    # torch.manual_seed(np.random.randint(sys.maxsize))\n",
        "    state = np.random.RandomState(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "H8DkDaKVTHp8"
      },
      "outputs": [],
      "source": [
        "# noise and class vector sizes\n",
        "noise_size = 128\n",
        "class_size = 128 if initial_class.lower()=='random embeddings' else 1000\n",
        "\n",
        "# load CLIP model unless we just used it\n",
        "if clip_model != last_clip_model:\n",
        "  perceptor, preprocess = clip.load(clip_model)\n",
        "  last_clip_model = clip_model\n",
        "\n",
        "# image resolution, model name\n",
        "channels = 3 if color else 1\n",
        "# clip_res = perceptor.input_resolution.item()\n",
        "clip_res = perceptor.visual.input_resolution\n",
        "sideX = sideY = int(size)\n",
        "gen_model = gen_model + '-' + size\n",
        "\n",
        "# load BigGAN model unless we just used it\n",
        "if gen_model != last_gen_model and 'biggan' in gen_model:\n",
        "  biggan_model = BigGAN.from_pretrained(gen_model).cuda().eval()\n",
        "  last_gen_model = gen_model\n",
        "\n",
        "# is our image smaller than the clip perceptor?\n",
        "if sideX<=clip_res and sideY<=clip_res:\n",
        "  augmentations = 1\n",
        "\n",
        "# for CMA we produce a population of candidate vectors, otherwise just 1 at a time\n",
        "if 'CMA' not in optimizer:\n",
        "  pop_size = 1\n",
        "\n",
        "# do not optimize class if not using BigGAN\n",
        "if 'biggan' not in gen_model:\n",
        "  optimize_class = False\n",
        "\n",
        "# BigGAN ImageNet class names to WordNet synsets/lemmas\n",
        "ind2name = {index: wn.of2ss('%08dn'%offset).lemma_names()[0] for offset, index in utils.IMAGENET.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24HSHGwlTHp8"
      },
      "outputs": [],
      "source": [
        "def replace_to_inplace_relu(model): #saves memory; from https://github.com/minyoungg/pix2latent/blob/master/pix2latent/model/biggan.py\n",
        "    for child_name, child in model.named_children():\n",
        "        if isinstance(child, nn.ReLU):\n",
        "            setattr(model, child_name, nn.ReLU(inplace=False))\n",
        "        else:\n",
        "            replace_to_inplace_relu(child)\n",
        "    return\n",
        "\n",
        "replace_to_inplace_relu(biggan_model)\n",
        "replace_to_inplace_relu(perceptor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUJ2S7xATHp8"
      },
      "source": [
        "Image and vectors saving helpers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USj-S8mhTHp8"
      },
      "outputs": [],
      "source": [
        "def save(out,name=None):\n",
        "  with torch.no_grad():\n",
        "    out = out.cpu().numpy()\n",
        "  img = convert_to_images(out)[0]\n",
        "  if name:\n",
        "    imageio.imwrite(name, np.asarray(img))\n",
        "  return img\n",
        "\n",
        "def save_vec(out,name):\n",
        "  with torch.no_grad():\n",
        "    vec = out.cpu().numpy()\n",
        "  np.savetxt(name, vec)\n",
        "\n",
        "def save_all_vecs(out, name):\n",
        "  with torch.no_grad():\n",
        "    vec = out.cpu().numpy()\n",
        "  np.savetxt(name, vec)\n",
        "\n",
        "hist = []\n",
        "def checkin(i, best_ind, total_losses, losses, regs, out, noise=None, emb=None, probs=None):\n",
        "  global sample_num, hist\n",
        "  name = None\n",
        "  if save_every and i%save_every==0:\n",
        "    name = '%s/frame_%05d.jpg'%(outpath, sample_num)\n",
        "  pil_image = save(out, name)\n",
        "  vals0 = [sample_num, i, total_losses[best_ind], losses[best_ind], regs[best_ind], np.mean(total_losses), np.mean(losses), np.mean(regs), np.std(total_losses), np.std(losses), np.std(regs)]\n",
        "  stats = 'sample=%d iter=%d best: total=%.2f cos=%.2f reg=%.3f avg: total=%.2f cos=%.2f reg=%.3f std: total=%.2f cos=%.2f reg=%.3f'%tuple(vals0)\n",
        "  vals1 = []\n",
        "  if noise is not None:\n",
        "    vals1 = [np.mean(noise), np.std(noise)]\n",
        "    stats += ' noise: avg=%.2f std=%.3f'%tuple(vals1)\n",
        "  vals2 = []\n",
        "  if emb is not None:\n",
        "    vals2 = [emb.mean(),emb.std()]\n",
        "    stats += ' emb: avg=%.2f std=%.3f'%tuple(vals2)\n",
        "  elif probs:\n",
        "    best = probs[best_ind]\n",
        "    inds = np.argsort(best)[::-1]\n",
        "    probs = np.array(probs)\n",
        "    vals2 = [ind2name[inds[0]], best[inds[0]], ind2name[inds[1]], best[inds[1]], ind2name[inds[2]], best[inds[2]], np.sum(probs >= 0.5)/pop_size,np.sum(probs >= 0.3)/pop_size,np.sum(probs >= 0.1)/pop_size]\n",
        "    stats += ' 1st=%s(%.2f) 2nd=%s(%.2f) 3rd=%s(%.2f) components: >=0.5:%.0f, >=0.3:%.0f, >=0.1:%.0f'%tuple(vals2)\n",
        "  hist.append(vals0+vals1+vals2)\n",
        "  if show_every and i%show_every==0:\n",
        "    clear_output()\n",
        "    display(pil_image)\n",
        "  print(stats)\n",
        "  print('Best index: %s' % best_ind)\n",
        "\n",
        "  # save best vectors\n",
        "  save_vec(noise_vector[best_ind], outpath+'noise_%05d.txt'%sample_num)\n",
        "  save_vec(class_vector[best_ind], outpath+'class_%05d.txt'%sample_num)\n",
        "  sample_num += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUYM_OCYTHp8"
      },
      "source": [
        "GAN generation helpers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxoR3XhRTHp8"
      },
      "outputs": [],
      "source": [
        "def get_output(noise_vector, class_vector):\n",
        "  save_class_vector_norm = None\n",
        "  if 'sigmoid' in gen_model:\n",
        "    out = noise_vector.sigmoid().reshape(1, channels, sideY, sideX)*2-1\n",
        "  else:\n",
        "    if stochastic_truncation: #https://arxiv.org/abs/1702.04782\n",
        "      with torch.no_grad():\n",
        "        trunc_indices = noise_vector.abs() > 2*truncation\n",
        "        size = torch.count_nonzero(trunc_indices).cpu().numpy()\n",
        "        trunc = truncnorm.rvs(-2*truncation, 2*truncation, size=(1,size)).astype(np.float32)\n",
        "        noise_vector.data[trunc_indices] = torch.tensor(trunc, requires_grad=requires_grad, device='cuda')\n",
        "    else:\n",
        "      noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
        "    if initial_class.lower() == 'random embeddings':\n",
        "      class_vector_norm = class_vector*emb_factor\n",
        "    else:\n",
        "      class_vector_norm = torch.softmax(class_vector/softmax_temp,dim=-1)\n",
        "    out = biggan_model(noise_vector, class_vector_norm, truncation)\n",
        "    if channels==1:\n",
        "      out = out.mean(dim=1, keepdim=True)\n",
        "    if initial_class.lower() != 'random embeddings':\n",
        "      save_class_vector_norm = class_vector_norm\n",
        "  if channels==1:\n",
        "    out = out.repeat(1,3,1,1)\n",
        "  return out, save_class_vector_norm\n",
        "\n",
        "# define forward pass\n",
        "def my_forward(self, z, class_label, truncation):\n",
        "  assert 0 < truncation <= 1\n",
        "\n",
        "  if initial_class.lower()=='random embeddings':\n",
        "    embed = class_label\n",
        "  else:\n",
        "    embed = self.embeddings(class_label)\n",
        "\n",
        "  cond_vector = torch.cat((z, embed), dim=1)\n",
        "\n",
        "  z = self.generator(cond_vector, truncation)\n",
        "  return z\n",
        "\n",
        "# set forward pass\n",
        "if gen_model == 'biggan':\n",
        "    BigGAN.forward = my_forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnzGln7ETHp8"
      },
      "source": [
        "Text optimization helpers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxLCKlrJTHp8"
      },
      "outputs": [],
      "source": [
        "def normality_loss(vec): #https://arxiv.org/abs/1903.00925\n",
        "    mu2 = vec.mean().square()\n",
        "    sigma2 = vec.var()\n",
        "    return mu2+sigma2-torch.log(sigma2)-1\n",
        "\n",
        "def make_safe_filename(s):\n",
        "    def safe_char(c):\n",
        "        if c.isalnum():\n",
        "            return c\n",
        "        else:\n",
        "            return \"_\"\n",
        "    return \"\".join(safe_char(c) for c in s).rstrip(\"_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wh2AZT9THp8"
      },
      "outputs": [],
      "source": [
        "requires_grad = ('SGD' in optimizer or 'Adam' in optimizer) and ('terminal' not in optimizer or terminal_iterations>0)\n",
        "total_iterations = iterations + terminal_iterations*('terminal' in optimizer)\n",
        "\n",
        "# ====\n",
        "# NOT EDITED BELOW\n",
        "\n",
        "for prompt in prompts:\n",
        "\n",
        "    # initialization\n",
        "    eps = 1e-8\n",
        "    class_vector = None\n",
        "    if 'sigmoid' in gen_model:\n",
        "      noise_size = channels*sideY*sideX\n",
        "      noise_vector = np.random.rand(pop_size, noise_size).astype(np.float32)\n",
        "      noise_vector = np.log((noise_vector+eps)/(1-noise_vector+eps))\n",
        "    else:\n",
        "      noise_vector = truncnorm.rvs(-2*truncation, 2*truncation, size=(pop_size, noise_size), random_state=state).astype(np.float32) #see https://github.com/tensorflow/hub/issues/214\n",
        "\n",
        "      if initial_class.lower() == 'random class':\n",
        "        class_vector = np.ones(shape=(pop_size, class_size), dtype=np.float32)*class_smoothing/999\n",
        "        class_vector[0,np.random.randint(class_size)] = 1-class_smoothing\n",
        "      elif initial_class.lower() == 'random dirichlet':\n",
        "        class_vector = dirichlet.rvs([pop_size/class_size] * class_size, size=1, random_state=state).astype(np.float32)\n",
        "      elif initial_class.lower() == 'random mix':\n",
        "        class_vector = np.random.rand(pop_size, class_size).astype(np.float32)\n",
        "      elif initial_class.lower() == 'random embeddings':\n",
        "        class_vector = np.random.randn(pop_size, class_size).astype(np.float32)\n",
        "      else:\n",
        "        if initial_class.lower() == 'from prompt':\n",
        "          initial_class = prompt\n",
        "        try:\n",
        "          class_vector = None\n",
        "          class_vector = one_hot_from_names(initial_class, batch_size=pop_size)\n",
        "          assert class_vector is not None\n",
        "          class_vector = class_vector*(1-class_smoothing*class_size/(class_size-1))+class_smoothing/(class_size-1)\n",
        "        except Exception as e:\n",
        "          print('Error: could not find initial_class. Try something else.')\n",
        "          raise e\n",
        "\n",
        "      if initial_class.lower() != 'random embeddings':\n",
        "        class_vector = class_vector/np.sum(class_vector,axis=-1, keepdims=True)\n",
        "        class_vector = np.log(class_vector+eps)-np.mean(np.log(class_vector+eps),axis=-1, keepdims=True)\n",
        "      initial_class_vector = class_vector[0]\n",
        "      if initial_class.lower() in ('random mix','random embeddings'):\n",
        "        initial_class_vector = initial_class_vector*0\n",
        "      class_vector = torch.tensor(class_vector, requires_grad=requires_grad, device='cuda')\n",
        "      smoothed_ent = -torch.tensor(class_smoothing*np.log(class_smoothing/999+eps)+(1-class_smoothing)*np.log(1-class_smoothing+eps), dtype=torch.float32).cuda()\n",
        "    noise_vector = torch.tensor(noise_vector, requires_grad=requires_grad, device='cuda')\n",
        "\n",
        "    if requires_grad:\n",
        "      params = [noise_vector]\n",
        "      if optimize_class:\n",
        "        params = params + [class_vector]\n",
        "      if 'SGD' in optimizer:\n",
        "        optim = torch.optim.SGD(params, lr=learning_rate, momentum=0.9)\n",
        "      else:\n",
        "        optim = torch.optim.Adam(params, lr=learning_rate)\n",
        "\n",
        "    # convert prompt to tokenized target\n",
        "    tx = clip.tokenize(prompt)\n",
        "    with torch.no_grad():\n",
        "      target_clip = perceptor.encode_text(tx.cuda())\n",
        "\n",
        "    # store best results as we optimize this prompt\n",
        "    global_best_loss = np.inf\n",
        "    global_best_iteration = 0\n",
        "    global_best_noise_vector = None\n",
        "    global_best_class_vector = None\n",
        "\n",
        "    # ascend the text\n",
        "    def ascend_txt(i, grad_step=False, show_save=False):\n",
        "      global global_best_loss, global_best_iteration, global_best_noise_vector, global_best_class_vector\n",
        "      prev_class_vector_norms = []\n",
        "      regs = []\n",
        "      losses = []\n",
        "      total_losses = []\n",
        "      best_loss = np.inf\n",
        "      global_reg = torch.tensor(0, device='cuda', dtype=torch.float32, requires_grad=grad_step)\n",
        "      if 'biggan' in gen_model:\n",
        "        if optimize_class and embed_normality_loss and initial_class.lower() == 'random embeddings':\n",
        "          global_reg = global_reg+embed_normality_loss*normality_loss(class_vector)\n",
        "        if noise_normality_loss:\n",
        "          global_reg = global_reg+noise_normality_loss*normality_loss(noise_vector)\n",
        "        global_reg = loss_factor*global_reg\n",
        "        if grad_step:\n",
        "          global_reg.backward()\n",
        "      for j in range(pop_size):\n",
        "        p_s = []\n",
        "        out, class_vector_norm = get_output(noise_vector[j:j+1], None if class_vector is None else class_vector[j:j+1])\n",
        "        if class_vector_norm is not None:\n",
        "          with torch.no_grad():\n",
        "            prev_class_vector_norms.append(class_vector_norm.cpu().numpy()[0])\n",
        "\n",
        "        for aug in range(augmentations):\n",
        "          if sideX<=clip_res and sideY<=clip_res or augmentations==1:\n",
        "            apper = out\n",
        "          else:\n",
        "            size = torch.randint(int(.7*sideX), int(.98*sideX), ())\n",
        "            offsetx = torch.randint(0, sideX - size, ())\n",
        "            offsety = torch.randint(0, sideX - size, ())\n",
        "            apper = out[:, :, offsetx:offsetx + size, offsety:offsety + size]\n",
        "          apper = (apper+1)/2\n",
        "          apper = nn.functional.interpolate(apper, clip_res, mode='bilinear')\n",
        "          #apper = apper.clamp(0,1)\n",
        "          p_s.append(apper)\n",
        "        into = nom(torch.cat(p_s, 0))\n",
        "        predict_clip = perceptor.encode_image(into)\n",
        "        loss = loss_factor*(1-torch.cosine_similarity(predict_clip, target_clip).mean())\n",
        "\n",
        "#         print(\"tokens:\", tx)\n",
        "#         print(\"target clip:\", target_clip)\n",
        "#         print(\"predict clip:\",predict_clip)\n",
        "#         print(\"cosine_sim:\", torch.cosine_similarity(predict_clip, target_clip).mean())\n",
        "#         sys.exit()\n",
        "\n",
        "        total_loss = loss\n",
        "        regs.append(global_reg.item())\n",
        "        if 'sigmoid' in gen_model and total_variation_loss or 'biggan' in gen_model and optimize_class and minimum_entropy_loss and initial_class.lower() != 'random embeddings':\n",
        "          if 'sigmoid' in gen_model and total_variation_loss:\n",
        "            reg = total_variation_loss*((out[:, :, :-1, :] - out[:, :, 1:, :]).abs().mean() + (out[:, :, :, :-1] - out[:, :, :, 1:]).abs().mean())\n",
        "          elif 'biggan' in gen_model and optimize_class and minimum_entropy_loss and initial_class.lower() != 'random embeddings':\n",
        "            reg = minimum_entropy_loss*((-class_vector_norm*torch.log(class_vector_norm+eps)).sum()-smoothed_ent).abs()\n",
        "          reg = loss_factor*reg\n",
        "          total_loss = total_loss + reg\n",
        "          with torch.no_grad():\n",
        "            regs[-1] += reg.item()\n",
        "        with torch.no_grad():\n",
        "          losses.append(loss.item())\n",
        "          total_losses.append(total_loss.item()+global_reg.item())\n",
        "        if total_losses[-1]<best_loss:\n",
        "          best_loss = total_losses[-1]\n",
        "          best_ind = j\n",
        "          best_out = out\n",
        "          if best_loss < global_best_loss:\n",
        "            global_best_loss = best_loss\n",
        "            global_best_iteration = i\n",
        "            with torch.no_grad():\n",
        "              global_best_noise_vector = noise_vector[best_ind]\n",
        "              if class_vector is not None:\n",
        "                global_best_class_vector = class_vector[best_ind]\n",
        "\n",
        "        if grad_step:\n",
        "          total_loss.backward()\n",
        "\n",
        "      if grad_step:\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "      if show_save and (save_every and i % save_every == 0 or show_every and i % show_every == 0):\n",
        "        noise = None\n",
        "        emb = None\n",
        "        if 'biggan' in gen_model:\n",
        "          with torch.no_grad():\n",
        "            noise = noise_vector.cpu().numpy()\n",
        "            if initial_class.lower() == 'random embeddings':\n",
        "              emb = class_vector.cpu().numpy()\n",
        "        checkin(i, best_ind, total_losses, losses, regs, best_out, noise, emb, prev_class_vector_norms)\n",
        "      return total_losses, best_ind\n",
        "\n",
        "    nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    if 'CMA' in optimizer:\n",
        "      initial_vector = np.zeros(noise_size)\n",
        "      bounds = None\n",
        "      #if 'biggan' in gen_model and not stochastic_truncation:\n",
        "      #  bounds = [-2*truncation*np.ones(noise_size),2*truncation*np.ones(noise_size)]\n",
        "      if optimize_class:\n",
        "        initial_vector = np.hstack([initial_vector, initial_class_vector])\n",
        "        #if not stochastic_truncation:\n",
        "        #  bounds[0] = list(bounds[0]) + [None]*class_size\n",
        "        #  bounds[1] = list(bounds[1]) + [None]*class_size\n",
        "      cma_opts = {'popsize': pop_size, 'seed': np.nan, 'AdaptSigma': cma_adapt, 'CMA_diagonal': cma_diag, 'CMA_active': cma_active, 'CMA_elitist':cma_elitist, 'bounds':bounds}\n",
        "      cmaes = cma.CMAEvolutionStrategy(initial_vector, sigma0, inopts=cma_opts)\n",
        "\n",
        "    sample_num = 0\n",
        "    machine = !nvidia-smi -L\n",
        "    start = time()\n",
        "    for i in range(total_iterations):\n",
        "      if 'CMA' in optimizer and i<iterations:\n",
        "        with torch.no_grad():\n",
        "          cma_results = torch.tensor(cmaes.ask(), dtype=torch.float32).cuda()\n",
        "          if optimize_class:\n",
        "            noise_vector.data, class_vector.data = torch.split_with_sizes(cma_results, (noise_size, class_size), dim=-1)\n",
        "            class_vector.data = class_vector.data\n",
        "          else:\n",
        "            noise_vector.data = cma_results\n",
        "      if requires_grad and ('terminal' not in optimizer or i>=iterations):\n",
        "        losses, best_ind = ascend_txt(i, grad_step=True, show_save='CMA' not in optimizer or i>=iterations)\n",
        "        assert noise_vector.requires_grad and noise_vector.is_leaf and (not optimize_class or class_vector.requires_grad and class_vector.is_leaf), (noise_vector.requires_grad, noise_vector.is_leaf, class_vector.requires_grad, class_vector.is_leaf)\n",
        "      if 'CMA' in optimizer and i<iterations:\n",
        "        with torch.no_grad():\n",
        "          losses, best_ind = ascend_txt(i, show_save=True)\n",
        "          if i<iterations-1:\n",
        "            if optimize_class:\n",
        "              vectors = torch.cat([noise_vector,class_vector], dim=1)\n",
        "            else:\n",
        "              vectors = noise_vector\n",
        "            cmaes.tell(vectors.cpu().numpy(), losses)\n",
        "          elif 'terminal' in optimizer and terminal_iterations:\n",
        "            pop_size = 1\n",
        "            noise_vector[0] = global_best_noise_vector\n",
        "            if class_vector is not None:\n",
        "              class_vector[0] = global_best_class_vector\n",
        "      if save_every and i % save_every == 0 or show_every and i % show_every == 0:\n",
        "        print('took: %d secs (%.2f sec/iter) on %s. CUDA memory: %.1f GB'%(time()-start,(time()-start)/(i+1), machine[0], torch.cuda.max_memory_allocated()/1024**3))\n",
        "        print('global best iteration: %d' % global_best_iteration)\n",
        "        print('prompt: %s' % prompt)\n",
        "        #print('prompt: %s, target-CLIP: %s' % (prompt, target_clip))\n",
        "\n",
        "    prompt_safe = make_safe_filename(prompt)\n",
        "    out, _ = get_output(global_best_noise_vector.unsqueeze(0), None if global_best_class_vector is None else global_best_class_vector.unsqueeze(0))\n",
        "    name = '%s/%s.jpg'%(resultspath, prompt_safe)\n",
        "    pil_image = save(out,name)\n",
        "    save_vec(global_best_noise_vector, '%s/%s_noise.txt' % (resultspath, prompt_safe))\n",
        "    save_vec(global_best_class_vector, '%s/%s_class.txt' % (resultspath, prompt_safe))\n",
        "\n",
        "    display(pil_image)\n",
        "    print('best_loss=%.2f best_iter=%d'%(global_best_loss,global_best_iteration))\n",
        "\n",
        "    # move outputs to saved path with datestampt\n",
        "    newdir_timestamp = os.path.join(storedpath,prompt_safe+\"_\"+datetime.datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
        "    newdir = os.path.join(storedpath, prompt_safe)\n",
        "    !mv $outpath $newdir_timestamp\n",
        "    !ln -s $newdir_timestamp $newdir\n",
        "    !mkdir -p $outpath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKDRLElpTHqG"
      },
      "outputs": [],
      "source": [
        "print(newdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtTkOm9tTHqG"
      },
      "source": [
        "# Explanation\n",
        "\n",
        "What is happening?\n",
        "\n",
        "- It generates a population of candidate images.\n",
        "- It chooses one that minimizes difference to text-to-image model (CMA-EAS covariance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsTGK9PKTHqG"
      },
      "source": [
        "# Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zapiIqriTHqG"
      },
      "source": [
        "Based on SIREN+CLIP Colabs by: [@advadnoun](https://twitter.com/advadnoun), [@norod78](https://twitter.com/norod78)\n",
        "\n",
        "Using the works:\n",
        "- https://github.com/openai/CLIP\n",
        "- https://tfhub.dev/deepmind/biggan-deep-512\n",
        "- https://github.com/huggingface/pytorch-pretrained-BigGAN\n",
        "- http://www.aiartonline.com/design-2019/eyal-gruss (WanderGAN)\n",
        "- Other CLIP notebooks: https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais\n",
        "- A curated list of more online generative tools see: [j.mp/generativetools](https://j.mp/generativetools)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "WanderCLIP.ipynb",
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (torch-gpu-clip)",
      "language": "python",
      "name": "torch-gpu-clip"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}