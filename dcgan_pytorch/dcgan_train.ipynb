{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMAR349 ML for the Arts - Twomey - Spring 2022 - [ml.roberttwomey.com](http://ml.roberttwomey.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch DCGAN implementation\n",
    "\n",
    "__NOTE for OOD users__: select the `tf-gpu-yourtts` kernel. (It has a recent torch and torchvision, which this requires)\n",
    "\n",
    "Forked from here: https://github.com/pytorch/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    npimg = npimg-np.amin(npimg)\n",
    "    npimg = npimg/np.amax(npimg)\n",
    "    plt.figure(figsize=(11,11))\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.axis(\"off\")\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save the results here\n",
    "outf=\"./dcgan_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manualSeed = random.randint(1, 10000)\n",
    "manualSeed = 4532\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "\n",
    "torch.manual_seed(manualSeed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "use_cuda=True\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "if torch.cuda.is_available() and not use_cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to download an image dataset and put them into a folder.\n",
    "The \"flowers\" dataset, for example, can be found here:\n",
    "http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
    "Usually you need at least a few thousand images to get reasonable results\n",
    "\n",
    "I already downloaded all of these images to our shared class folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageSize = 64 # square images for now!\n",
    "# dataroot = \"./flowers\"\n",
    "dataroot = \"/home/emar349/shared/dcgan/flowers\"\n",
    "#dataroot = \"./geom_pics_dataset\"\n",
    "\n",
    "# folder dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Scale(imageSize),\n",
    "                                   transforms.CenterCrop(imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize= 64\n",
    "workers = 2\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=int(workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num of batches:', dataloader.__len__())\n",
    "print('num of images:', dataloader.__len__()*batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this example demonstrates how we can load a new batch of data\n",
    "dataiter = iter(dataloader)\n",
    "data=dataiter.next()\n",
    "data[0]=(data[0]+1)/2\n",
    "print('len of data:',len(data))\n",
    "print('shape of data:', data[0].size(),data[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels are in data[1][:]\n",
    "print('labels:',data[1][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some examples from the original (_real_) data\n",
    "# images are in data[0][:]\n",
    "show(make_grid(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu = int(1) # use one GPU\n",
    "nz = int(100) # code dimension (This is the() random noise) input dimension of the generator network)\n",
    "ngf = int(64) # output dimension of the generator network\n",
    "ndf = int(64) # input dim (image size) for the discriminator net\n",
    "nc = 3 # number of input channels (e.g. 3 for RGB channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create the Generator network\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(_netG, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = _netG(ngpu)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# if we want to load a saved netG, we can do it here:\n",
    "#savednetG=outf+\"/netG_epoch_39.pth\"\n",
    "#netG.load_state_dict(torch.load(savednetG))\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create the Discriminator network\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(_netD, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = _netD(ngpu)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# If we want to load a saved netD, we can do it here:\n",
    "#savednetD=outf+\"/netD_epoch_39.pth\"\n",
    "#netD.load_state_dict(torch.load(savednetD))\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy between the target (p, 1-p) and the output (q,1-q) distributions\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.FloatTensor(batchSize, 3, imageSize, imageSize) # this will be the input of the Discriminator network\n",
    "\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1) # this will be the input of the Generator network\n",
    "\n",
    "# We will also use some \"fixed\" noise to monitor the progess of the generated data\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "\n",
    "label = torch.FloatTensor(batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put NetD, netG, the criterion, input, and noise to the GPUs\n",
    "if use_cuda:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    criterion.cuda()\n",
    "    input, label = input.cuda(), label.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = Variable(fixed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "\n",
    "beta1 =0.5\n",
    "lr=0.0002\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr, betas=(beta1, 0.999))\n",
    "\n",
    "#optimizerD = optim.SGD(netD.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizerG = optim.SGD(netG.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will show one step of how to optimize NetG and NetD\n",
    "Then we will put these steps into a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "###########################\n",
    "# train with real\n",
    "netD.zero_grad()\n",
    "real_cpu, _ = data # data[0] = images, data[1] =labels (triangle, ellipse, rectangle). Labels are not important\n",
    "batch_size = real_cpu.size(0)\n",
    "if use_cuda:\n",
    "    real_cpu = real_cpu.cuda() # move the image data to the GPUs\n",
    "input.resize_as_(real_cpu).copy_(real_cpu) #copy the image data to input variable\n",
    "print(\"input:\", input.size())\n",
    "label.resize_(batch_size).fill_(real_label) #since this is all real data add them \"real_label\" label\n",
    "inputv = Variable(input)\n",
    "labelv = Variable(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = netD(inputv) # caclulate the predicitions of netD network:\n",
    "output.size()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[0:5], labelv[0:5]) # We want the predicited output to be all ones (real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the output of netD(real images) to be all ones.\n",
    "# Let us caclulate the error\n",
    "errD_real = criterion(output, labelv)\n",
    "# We want the error to be as small as possible\n",
    "print(\"Error of Discriminator on real data: \",errD_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let D_x be the mean of the outputs on real data. \n",
    "#Since we used real data, we want D_x to be close to 1\n",
    "D_x = output.data.mean()\n",
    "print(D_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us caculate the gradient of errD_real. \n",
    "errD_real.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will train with fake data\n",
    "noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "noisev = Variable(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generate fake images from noise\n",
    "fake = netG(noisev)\n",
    "fake.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \"fake_label\" labels to the fake images\n",
    "labelv = Variable(label.fill_(fake_label))\n",
    "labelv.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = netD(fake.detach())\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want the output of netD(fake images) to be all zeros\n",
    "print(output[0:5], labelv[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the error of the Discriminiator network on the fake data\n",
    "errD_fake = criterion(output, labelv)\n",
    "errD_fake\n",
    "# We want this error to be as small as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient of errD_fake\n",
    "errD_fake.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let D_G_z1 be the mean of the output of the Discriminator on fake data\n",
    "# We want D_G_z1 to be close to 0\n",
    "D_G_z1 = output.data.mean()\n",
    "D_G_z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined error:\n",
    "errD = errD_real + errD_fake\n",
    "errD\n",
    "# We want this to be as close to zero as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Update D network: maximize log(D(x)) + log(1 - D(G(z))) \n",
    " We want D to be as good as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Update G network: maximize log(D(G(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Generator wants to trick the Discriminator\n",
    "labelv = Variable(label.fill_(real_label))  # fake labels are real for generator cost\n",
    "labelv.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = netD(fake)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[0:5], labelv[0:5]) #We want the generator the trick the Discriminator, i.e. produce all ones here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want this to be as small as possible\n",
    "errG = criterion(output, labelv)\n",
    "errG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errG.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want this to be as close to one as possible (we want good fake data)\n",
    "D_G_z2 = output.data.mean()\n",
    "D_G_z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerG.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalutation to see if netG optimizer helped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output = netD(netG(noisev))\n",
    "# we want these to be all ones\n",
    "new_errG = criterion(new_output, labelv)\n",
    "# We want the error to be as small as possible\n",
    "new_errG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want this to be as close to one as possible (we want good fake data)\n",
    "new_D_G_z2 = new_output.data.mean()\n",
    "new_D_G_z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want these to be all ones\n",
    "print(new_output, labelv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 5\n",
    "epoch=1\n",
    "i=1\n",
    "print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, niter, i, len(dataloader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(make_grid(real_cpu.cpu())) #show some real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = netG(fixed_noise)\n",
    "fake.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(make_grid(fake.data.cpu())) # show some fake data. It should look bad for an untrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below loop might take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda=True\n",
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu, _ = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "        if use_cuda:\n",
    "            real_cpu = real_cpu.cuda()\n",
    "        input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "        label.resize_(batch_size).fill_(real_label)\n",
    "        inputv = Variable(input)\n",
    "        labelv = Variable(label)\n",
    "\n",
    "        output = netD(inputv)\n",
    "        errD_real = criterion(output, labelv)\n",
    "        errD_real.backward()\n",
    "        D_x = output.data.mean()\n",
    "\n",
    "        # train with fake\n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        labelv = Variable(label.fill_(fake_label))\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, labelv)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        labelv = Variable(label.fill_(real_label))  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, labelv)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.data.mean()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, niter, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        \n",
    "\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,'%s/real_samples.png' % outf,normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            #show(make_grid(fake.data.cpu()))\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (outf, epoch),\n",
    "                    normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do checkpointing\n",
    "torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outf, epoch))\n",
    "torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outf, epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "noise = noise.cuda()\n",
    "noisev = Variable(noise)\n",
    "fake = netG(noisev)\n",
    "fake.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake=fake.cpu()\n",
    "show(make_grid(fake.data, padding=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activities\n",
    "- Interpolate between two difference noise vectors\n",
    "    - spherical or linear interpolation\n",
    "- Experiment with a different dataset. \n",
    "    - See CelebA dataset in this tutorial: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "    - LSUN dataset (bedrooms, cars, etc.): https://github.com/pytorch/examples/tree/main/dcgan#downloading-the-dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu-yourtts)",
   "language": "python",
   "name": "tf-gpu-yourtts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
