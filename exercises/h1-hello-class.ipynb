{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYU6phu2ceUt"
      },
      "source": [
        "# H1 Hello Class!\n",
        "Machine Learning for the Arts | Prof. Twomey | [ml.roberttwomey.com](https://ml.roberttwomey.com)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook has two sections.\n",
        "\n",
        "[Section 1: Working with Markdown](https://colab.research.google.com/github/roberttwomey/ml-art-code/blob/master/exercises/h1-watch-me-learn.ipynb#scrollTo=iB-V7YYh0ZDa&line=7&uniqifier=1) asks you to practice with a little bit of markdown styling.\n",
        "\n",
        "[Section 2: Watch Me Learn](https://colab.research.google.com/github/roberttwomey/ml-art-code/blob/master/exercises/h1-hello-class.ipynb#scrollTo=fwBSSIin23n7&line=7&uniqifier=1) has you pick a text file to use as training data, and then train a minimal little language model from scratch. This part is based on a character-level model written by Andrej Karpathy (@karpathy). We will learn more about what an [RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network) is later.\n",
        "\n",
        "When you are done with Section 1 and Section 2, export your file as a jupyter notebook (.ipynb), and also save a copy as a PDF. Submit this as Homework 1 on Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Working with Markdown\n",
        "\n",
        "## 1. First off, let's make this notebook your own!\n",
        "\n",
        "Go up to the top cell of this notebook and change the title to something that you like better\n",
        "\n",
        "Under the title and above the overview section, add in a block of text that says your name, email, and some other info of your choice. Try playing around with the markdown styling, html.\n",
        "\n",
        "## 2. Add a text cell below this one\n",
        "Insert an image in that cell. Include some text of your choice.\n",
        "\n",
        "## 3. Add a python (code) cell below that.\n",
        "Write some python code that does something. A good place to start might be just a print statement:\n",
        "\n",
        "```python\n",
        "print(\"Hello Class!\")\n",
        "```\n",
        "\n",
        "## 4. Add a variable to your code cell.\n",
        "\n",
        "Try some code like the following (you will need to copy it into a new code cell).\n",
        "\n",
        "```python\n",
        "x = \"roberto\"\n",
        "```\n",
        "or\n",
        "\n",
        "```python\n",
        "the_answer = 10\n",
        "```\n",
        "\n",
        "## 5. Add one more code cell where you print the value of that variable:\n",
        "\n",
        "```python\n",
        "print(x)\n",
        "print(the_answer)\n",
        "```\n",
        "\n",
        "## End of Section 1\n",
        "Congrats, that's the end of section 1."
      ],
      "metadata": {
        "id": "iB-V7YYh0ZDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Watch Me Learn\n",
        "Here we are going to do a little machine learning. Just two parts:\n",
        "\n",
        "1. Download a text file that you want the language model to learn from.\n",
        "2. Train the language model and watch it learn from your text file.\n",
        "\n",
        "This corresponds to the two parts of a typical ML project: **finding training data** and **training a model**.\n",
        "\n",
        "NOTE: The language model below is a character-level Recurrent Neural Network (RNN) written by Andrej Karpathy. It is only 100 lines of code! Source: [https://gist.github.com/karpathy/d4dee566867f8291f086](https://gist.github.com/karpathy/d4dee566867f8291f086)"
      ],
      "metadata": {
        "id": "fwBSSIin23n7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Gather Training Data\n",
        "\n",
        "You need to download a text file to work with.\n",
        "\n",
        "First, find a text file online that you want to use for this project. (Or grab a text file that you already own). This should be a plain text file (ending in .txt), i.e. not a Microsoft Word file, pdf, or anything like that.\n",
        "\n",
        "In the cell below, replace the URL (**https://raw.github...**) with the address to a text file you want the model to learn from.\n",
        "\n",
        "Then run the cell. This will download the file to the current directory with your notebook.\n",
        "\n",
        "**NOTES**:\n",
        "- You can click on the files icon at left to see the files in your current directory.\n",
        "- From our intro notebooks, you know that the `!` runs a command on the computer that is running this notebook.\n",
        "- **wget** is a program on most linux-based computers that downloads a filew from the internet.\n",
        "- So **!wget https://raw.github...** runs a command to download a file from the internet to our current directory."
      ],
      "metadata": {
        "id": "WlZK0pR8yzwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/roberttwomey/ml-art-code/master/intro/script.txt"
      ],
      "metadata": {
        "id": "Fv1BqQElc78B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "^^^ Check out the output above ^^^. Can you make sense of it? What do you think happened..."
      ],
      "metadata": {
        "id": "iQNEATZS5Zmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Source Code for the RNN\n",
        "This was written by Andrej Karpathy. Do you know who he is?\n",
        "\n",
        "Run the following cell to run the basic imports and declare the helper functions for our Recurrent Neural Network."
      ],
      "metadata": {
        "id": "OaQwdujM5jCP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG4fj138ceU3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# data I/O\n",
        "data = open('script.txt', 'r', encoding ='ISO-8859-1').read() # should be simple plain text file\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# hyperparameters\n",
        "hidden_size = 100 # size of hidden layer of neurons\n",
        "seq_length = 25 # number of steps to unroll the RNN for\n",
        "learning_rate = 1e-1\n",
        "\n",
        "# model parameters\n",
        "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
        "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
        "by = np.zeros((vocab_size, 1)) # output bias\n",
        "\n",
        "def lossFun(inputs, targets, hprev):\n",
        "  \"\"\"\n",
        "  inputs,targets are both list of integers.\n",
        "  hprev is Hx1 array of initial hidden state\n",
        "  returns the loss, gradients on model parameters, and last hidden state\n",
        "  \"\"\"\n",
        "  xs, hs, ys, ps = {}, {}, {}, {}\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  loss = 0\n",
        "  # forward pass\n",
        "  for t in range(len(inputs)):\n",
        "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
        "    xs[t][inputs[t]] = 1\n",
        "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
        "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
        "  # backward pass: compute gradients going backwards\n",
        "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "  for t in reversed(range(len(inputs))):\n",
        "    dy = np.copy(ps[t])\n",
        "    dy[targets[t]] -= 1 # backprop into y. see\n",
        "\n",
        "    #if confused here\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    dby += dy\n",
        "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
        "    dbh += dhraw\n",
        "    dWxh += np.dot(dhraw, xs[t].T)\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "    dhnext = np.dot(Whh.T, dhraw)\n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
        "\n",
        "def sample(h, seed_ix, n):\n",
        "  \"\"\"\n",
        "  sample a sequence of integers from the model\n",
        "  h is memory state, seed_ix is seed letter for first time step\n",
        "  \"\"\"\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  x[seed_ix] = 1\n",
        "  ixes = []\n",
        "  for t in range(n):\n",
        "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "    y = np.dot(Why, h) + by\n",
        "    p = np.exp(y) / np.sum(np.exp(y))\n",
        "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[ix] = 1\n",
        "    ixes.append(ix)\n",
        "  return ixes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see a message about your data if the cell ran properly. (f.ex: `data has 82463 characters, 95 unique.`)\n",
        "\n",
        "The next cell declares some variables and will actually run the training loop.\n",
        "\n",
        "**Run the following cell**.\n",
        "\n",
        "Watch it run for a while and see how the outputs change. This will run **forever**. At some point you should click stop and then export your .ipynb and your .pdf to submit for the homework."
      ],
      "metadata": {
        "id": "ai8gN5me6HDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
        "while True:\n",
        "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "  if p+seq_length+1 >= len(data) or n == 0:\n",
        "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    p = 0 # go from start of data\n",
        "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "  # sample from the model now and then\n",
        "  if n % 1000 == 0:\n",
        "    sample_ix = sample(hprev, inputs[0], 100)\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    print('----\\n %s \\n----' % (txt, ))\n",
        "\n",
        "  # forward seq_length characters through the net and fetch gradient\n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "  if n % 1000 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
        "\n",
        "  # perform parameter update with Adagrad\n",
        "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
        "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "  p += seq_length # move data pointer\n",
        "  n += 1 # iteration counter"
      ],
      "metadata": {
        "id": "9mMOhQLc52Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Watch it run for a while and see how the outputs change. **Answer the following questions in the empty text box below**:\n",
        "1. How many iterations did you getup to? This is a count of how many training examples you showed to the network.\n",
        "2. What is the final loss for your network? (**loss**). Where did the loss start? The loss function, or loss score, is the main measure of learning in a neural network.\n",
        "3. How do the outputs look at the end? Is the text intelligible? Does it make sense? Does the output sample resemble the training data that you showed to the network?"
      ],
      "metadata": {
        "id": "sjJBGW6764eK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wr1YsgXE7mzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End of Section 2\n",
        "\n",
        "Don't forget to submit your homework on Canvas.\n",
        "\n",
        "Export your file as a jupyter notebook (.ipynb), and also save a copy as a PDF. Give them descriptive filenames (*Twomey-H1.ipynb*, and *Twomey-H1.pdf* for instance but with your last name).\n",
        "\n",
        "Submit this as Homework 1 on Canvas."
      ],
      "metadata": {
        "id": "v39j6ViS7v60"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}