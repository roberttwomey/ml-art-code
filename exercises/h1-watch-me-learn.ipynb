{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYU6phu2ceUt"
      },
      "source": [
        "## Vanilla RNN\n",
        "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
        "\n",
        "BSD License\n",
        "\n",
        "Source: [https://gist.github.com/karpathy/d4dee566867f8291f086](https://gist.github.com/karpathy/d4dee566867f8291f086)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a text file to work with. In the cell below, replace the URL with a link to a text file you want the model to learn from then run the cell."
      ],
      "metadata": {
        "id": "WlZK0pR8yzwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/roberttwomey/ml-art-code/master/intro/script.txt"
      ],
      "metadata": {
        "id": "Fv1BqQElc78B",
        "outputId": "db5d09c2-e794-442e-a0f9-d9330b09c689",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-25 18:02:08--  https://raw.githubusercontent.com/roberttwomey/ml-art-code/master/intro/script.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82463 (81K) [text/plain]\n",
            "Saving to: ‘script.txt’\n",
            "\n",
            "\rscript.txt            0%[                    ]       0  --.-KB/s               \rscript.txt          100%[===================>]  80.53K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-01-25 18:02:08 (5.79 MB/s) - ‘script.txt’ saved [82463/82463]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG4fj138ceU3",
        "outputId": "112baeeb-0079-44f0-edb6-cf4d8f79ff92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 82463 characters, 95 unique.\n",
            "----\n",
            " $ot@ÓH('mV¨o2\tSÕÕXB\n",
            "aoaVÑÕkKlk;uDK4,u6L',R|cn30RFÑdd>¨0FbFC$V?A---m+Ðf@m!#t#V4qd^V4_1h$4j7br,2auv4@D \n",
            "----\n",
            "iter 0, loss: 113.846919\n",
            "----\n",
            " ko9cod an endorin adreleurtadikocrneat.  toa. onlrantyeb\n",
            "ronesrmmothilis nomoonweralooeeatye ang are \n",
            "----\n",
            "iter 1000, loss: 88.073080\n",
            "----\n",
            " iuo al\n",
            "ituins actuthife angleifidedic incriot Jactitiol cocreorysfsisu hal\n",
            "palfiocin\"cimay ang. \n",
            "\n",
            "Th \n",
            "----\n",
            "iter 2000, loss: 71.356878\n",
            "----\n",
            " ger dcpakudeqhpaqghy al that qumld thic0tourcalgus us in prorget cexpmofay keegkecamtyakur wofilsy a \n",
            "----\n",
            "iter 3000, loss: 68.624097\n",
            "----\n",
            " prumat the ithe loolle latory. \n",
            "\n",
            "iphedmechitu thukesebickalived ergenumsterlaml thice.  fsinging ter \n",
            "----\n",
            "iter 4000, loss: 63.535718\n",
            "----\n",
            " rche .\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "lishl uice pro be tiog wounge kare wes--les. .\n",
            "\n",
            "Thavillmo oad wars ar loosimans AnibDy ca \n",
            "----\n",
            "iter 5000, loss: 58.979158\n",
            "----\n",
            "  nike, in e d-artere.  nective rotoc. \n",
            "\n",
            "1iinteif caechitl thtte, thseched.  led th.\n",
            "mve anat.  n mit \n",
            "----\n",
            "iter 6000, loss: 60.476217\n",
            "----\n",
            " s.  I Whef soiv whe ruca copn ic I (litels doriin prerd pry. \n",
            "ine nac prcban feve mat oigema stK who \n",
            "----\n",
            "iter 7000, loss: 59.519451\n",
            "----\n",
            " hadtu, in beory ined alaring 1lat woth resally revirk trocat ot at if br ct.  ath. \n",
            "\n",
            "of I wackir.s s \n",
            "----\n",
            "iter 8000, loss: 56.532030\n",
            "----\n",
            " ) anser ir'ny Jiallire of an thiwe gnomemy is in suow unsifatirgot here e hie sthat alute kepatlis w \n",
            "----\n",
            "iter 9000, loss: 56.658940\n",
            "----\n",
            " t alat sreal at.\n",
            "\n",
            "a the this thotls.\" so mechen mexe o th the hesul mage oveal athod the the that me \n",
            "----\n",
            "iter 10000, loss: 58.744439\n",
            "----\n",
            " exesh or bet and asb de of at a gyce intestificher preting tist oby ow o bese ids \n",
            "\n",
            "wall m\n",
            "paralinge \n",
            "----\n",
            "iter 11000, loss: 56.067653\n",
            "----\n",
            " u delacals? nould.\n",
            "\n",
            "chion\n",
            "\n",
            "soucped. \n",
            "\n",
            "- oups ashulscbmy it of ard of pat aperelo xucklle al sevefpec \n",
            "----\n",
            "iter 12000, loss: 54.451556\n",
            "----\n",
            " ice pratitr be ins wepemcos made ule redt of ancessti, pare.\n",
            "\n",
            "the thisingpis fo race thave donk.\n",
            "cn, \n",
            "----\n",
            "iter 13000, loss: 58.034855\n",
            "----\n",
            " antisillinalory.  ifver pukcer stiyrathise ofs.  boyel conged rockes of wher fit inratereon)all expe \n",
            "----\n",
            "iter 14000, loss: 56.806599\n",
            "----\n",
            "  bay of dher addencrsek ids coussy.  s:scrd the do let AI dond prod fadd tidcat colty semty and west \n",
            "----\n",
            "iter 15000, loss: 54.660677\n",
            "----\n",
            " e:.. and do 6estio fo in my hages richme in eat nirch dore shing cacimonenture, at cont, is my naren \n",
            "----\n",
            "iter 16000, loss: 57.309820\n",
            "----\n",
            " s whand tha chicis in asdiges on souss toe shitite mabesion in to ny not it sine all_my d'cu I wine  \n",
            "----\n",
            "iter 17000, loss: 56.232843\n",
            "----\n",
            " for in Tn, IS made ming prtnhing oun a suve e to deem tale o pancoplith dat cal prod to ar.  I and o \n",
            "----\n",
            "iter 18000, loss: 53.611695\n",
            "----\n",
            " d in wish the Co hexy mide.  ang of IbMy luts thencale srod a racansirgice.\n",
            "\n",
            "Mater Ky tEXTeand.  Th  \n",
            "----\n",
            "iter 19000, loss: 54.849511\n",
            "----\n",
            " \n",
            "\n",
            "sever beam ha.\n",
            "\n",
            "br.\n",
            "\n",
            "I and the I Ifty utteritirk.\n",
            "\n",
            "Otexs thes wist of brituine the fonter there ar \n",
            "----\n",
            "iter 20000, loss: 56.134798\n",
            "----\n",
            " . \n",
            "\n",
            "sthigichidion wh have thhang I mar, waperis, your soplacios, rikee seme deces prod, you pulit pr \n",
            "----\n",
            "iter 21000, loss: 53.804387\n",
            "----\n",
            " leate- oO8 fahtercpad tnespsio. srater to \n",
            "\n",
            "sol ga art it uucfs as'thy so Cane on mdo sale.\n",
            "\n",
            "I'p moc \n",
            "----\n",
            "iter 22000, loss: 53.282442\n",
            "----\n",
            " interbor soigr of horoens olonalalA wist peal wonter of yovepor, reand. \n",
            "The - deyis on artrola sein \n",
            "----\n",
            "iter 23000, loss: 56.373614\n",
            "----\n",
            " nd wigh ithere stime that'd fint on kersen enon a wise nacilago1ntischectmallyingrlatiss fahing borm \n",
            "----\n",
            "iter 24000, loss: 53.816768\n",
            "----\n",
            " sut am\"ineses-2lear raf woell.  Sfukis. oncelet hast r.  Boow sis limef.  The gmang. \n",
            "\n",
            "sach a Lract  \n",
            "----\n",
            "iter 25000, loss: 52.225339\n",
            "----\n",
            " he ivendorescall dilac. rivulim lrowals proniegny sond: pofound.\n",
            "\n",
            "The -15, lank rapescatugh wads.\n",
            "\n",
            "W \n",
            "----\n",
            "iter 26000, loss: 55.362323\n",
            "----\n",
            " oed ic.  sofuck gos danLce mowhal do I cantiog. \n",
            "\n",
            "sahe rill becednd2pfapirlevide - no'nt-ading art.  \n",
            "----\n",
            "iter 27000, loss: 54.023309\n",
            "----\n",
            " wtyous atenter. \n",
            "\n",
            "eagresinvolyton't rareldery that lanke laliccerd inam ow leeciting. wonder a. bere \n",
            "----\n",
            "iter 28000, loss: 51.559045\n",
            "----\n",
            " eyfugens pakere to oos ance o ODCSout coit 8Myore it cak that thirguboerent, modrion.  and a miching \n",
            "----\n",
            "iter 29000, loss: 53.425683\n",
            "----\n",
            " ing.  sunvest interal to nacts bed deher go vortenf to wingiens, cadrabos.  reilallonangrieacriboet. \n",
            "----\n",
            "iter 30000, loss: 54.126036\n",
            "----\n",
            "  youncal.  a be dimploon.  I I (a digghing goed worbovectane voresing-plinc angend, to nuc aly cyels \n",
            "----\n",
            "iter 31000, loss: 51.793229\n",
            "----\n",
            " e\n",
            "thod on ctyhe\n",
            "\n",
            "have alwing)\n",
            "\n",
            "thas like my moresys, it this scuetrecinat hyperking.\n",
            "\n",
            "it nevery boy  \n",
            "----\n",
            "iter 32000, loss: 51.635968\n",
            "----\n",
            " ncial alye to cere. \n",
            "\n",
            "Lowhy withgich nhe, ader to dece to to that it: A2tiontionthergh, thy heve lat \n",
            "----\n",
            "iter 33000, loss: 54.771468\n",
            "----\n",
            " ecesthay this son the stode whabertctoecteschiy prkeragit broprily meane protitioctrris or omarsoonc \n",
            "----\n",
            "iter 34000, loss: 52.216649\n",
            "----\n",
            " ch.  TOhisills, ro thot ot, an't , ctrk, Whet is as a cor comedomy ster the  to\n",
            "\n",
            "thethabisiost.  cem \n",
            "----\n",
            "iter 35000, loss: 50.669296\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# data I/O\n",
        "data = open('script.txt', 'r', encoding ='ISO-8859-1').read() # should be simple plain text file\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# hyperparameters\n",
        "hidden_size = 100 # size of hidden layer of neurons\n",
        "seq_length = 25 # number of steps to unroll the RNN for\n",
        "learning_rate = 1e-1\n",
        "\n",
        "# model parameters\n",
        "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
        "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
        "by = np.zeros((vocab_size, 1)) # output bias\n",
        "\n",
        "def lossFun(inputs, targets, hprev):\n",
        "  \"\"\"\n",
        "  inputs,targets are both list of integers.\n",
        "  hprev is Hx1 array of initial hidden state\n",
        "  returns the loss, gradients on model parameters, and last hidden state\n",
        "  \"\"\"\n",
        "  xs, hs, ys, ps = {}, {}, {}, {}\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  loss = 0\n",
        "  # forward pass\n",
        "  for t in range(len(inputs)):\n",
        "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
        "    xs[t][inputs[t]] = 1\n",
        "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
        "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
        "  # backward pass: compute gradients going backwards\n",
        "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "  for t in reversed(range(len(inputs))):\n",
        "    dy = np.copy(ps[t])\n",
        "    dy[targets[t]] -= 1 # backprop into y. see\n",
        "\n",
        "    #if confused here\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    dby += dy\n",
        "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
        "    dbh += dhraw\n",
        "    dWxh += np.dot(dhraw, xs[t].T)\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "    dhnext = np.dot(Whh.T, dhraw)\n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
        "\n",
        "def sample(h, seed_ix, n):\n",
        "  \"\"\"\n",
        "  sample a sequence of integers from the model\n",
        "  h is memory state, seed_ix is seed letter for first time step\n",
        "  \"\"\"\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  x[seed_ix] = 1\n",
        "  ixes = []\n",
        "  for t in range(n):\n",
        "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "    y = np.dot(Why, h) + by\n",
        "    p = np.exp(y) / np.sum(np.exp(y))\n",
        "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[ix] = 1\n",
        "    ixes.append(ix)\n",
        "  return ixes\n",
        "\n",
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
        "while True:\n",
        "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "  if p+seq_length+1 >= len(data) or n == 0:\n",
        "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    p = 0 # go from start of data\n",
        "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "  # sample from the model now and then\n",
        "  if n % 1000 == 0:\n",
        "    sample_ix = sample(hprev, inputs[0], 100)\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    print('----\\n %s \\n----' % (txt, ))\n",
        "\n",
        "  # forward seq_length characters through the net and fetch gradient\n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "  if n % 1000 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
        "\n",
        "  # perform parameter update with Adagrad\n",
        "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
        "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "  p += seq_length # move data pointer\n",
        "  n += 1 # iteration counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SJqDwTmMce42"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}