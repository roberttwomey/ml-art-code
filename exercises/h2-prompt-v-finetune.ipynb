{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roberttwomey/ml-art-code/blob/master/exercises/h2-prompt-v-finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Prompting vs. Finetuning GPT-2\n",
        "\n",
        "The predominant (and encourged) way of interacting with Large Language Models ([LLMs](https://en.wikipedia.org/wiki/Large_language_model)) such as ChatGPT is through prompting. The idea with prompting an LLM is that the model already contains the flexibility to address a diverse array of requests, it is simply a matter of presenting it with a context that elicits the desired response.\n",
        "\n",
        "Prompting a model does not change anything about the model, it is simply a query on the pre-existing system. So to some extent you are working with what is \"given\", by OpenAI or whomever published your model of choice.\n",
        "\n",
        "Finetuning a model is a way of continuing its training, beyond what the researchers, corporation, etc. imbued in it, to specialize to a particular domain or textual corpus that is your choice.\n",
        "\n",
        "This exercise explores GPT-2 ([Generative Pretrained Transformer-2](https://openai.com/blog/better-language-models/)) from OpenAI from these perspectives of Prompting vs. Finetuning.\n",
        "\n",
        "Two parts:\n",
        "- 1. [Prompting](https://colab.research.google.com/drive/1757aNBs3olsvJK3nCM8U5ylR2hAc59Vp#scrollTo=QQAN3M6RT7Kj&line=3&uniqifier=1)\n",
        "- 2. [Finetuning](https://colab.research.google.com/drive/1757aNBs3olsvJK3nCM8U5ylR2hAc59Vp#scrollTo=uatZmJ2Pwl7g&line=11&uniqifier=1)\n",
        "\n",
        "Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2lXOa49wl7d"
      },
      "source": [
        "# 0. Setup your GPT-2 Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3--iFQpUwl7e"
      },
      "source": [
        "Run once to install the library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "As9SPgtjwl7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a718250-2212-4d59-86c1-98ff6f008f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZl6nl_Uwl7e"
      },
      "source": [
        "restart the kernel and run the imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KBkpRgBCBS2_"
      },
      "outputs": [],
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import tensorflow as tf\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Download GPT-2\n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80/P4 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the the current working directory at `/models/<model_name>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P8wSlgXoDPCR"
      },
      "outputs": [],
      "source": [
        "model_name = \"124M\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoMFQcmwwl7f"
      },
      "source": [
        "run once to download the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "thg7irbWwl7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b455c5d-494b-4d2c-f5f4-fd3ef506b5ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 3.98Git/s]                                                     \n",
            "Fetching encoder.json: 1.05Mit [00:00, 5.22Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 4.05Git/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:08, 61.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 3.64Git/s]                                               \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 6.58Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 6.92Mit/s]\n"
          ]
        }
      ],
      "source": [
        "gpt2.download_gpt2(model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj"
      },
      "source": [
        "# 1: Prompting\n",
        "\n",
        "GPT-2 is assuredly less-capable than ChatGPT in prompting, but the same basic principles underly the interaction: the idea of \"completion\". A prompt for GPT-2 is a string (prefix), which the language model then tries to continue/extend.\n",
        "\n",
        "First, load the GPT models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d5b08b-bace-4310-98d6-8a9470588fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained model models/124M/model.ckpt\n"
          ]
        }
      ],
      "source": [
        "# start session and load model\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb5wGHJkwl7g"
      },
      "source": [
        "## Sample from the model\n",
        "\n",
        "The follow cell samples from gpt-2, using the provided prefix (seed) and other parameters. It starts the TF session and generates the samples.\n",
        "\n",
        "Try changing the parameters below to change the output:\n",
        "- `prefix` is the prompt. This will be the starting string/seed for your generation. Use your own text.\n",
        "- `temperature` sets the variability/randomness of the output. Range 0.0-1.0\n",
        "- `length` sets the lenght of output (in tokens). max is 1024.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-xInIZKaU104",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13579eac-29e4-48f9-c102-a40ba114fcd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The secret of life is the other side. The secret of life is the other side.\"\n",
            "\n",
            "He continued: \"The other side is the other side. And we need to fight for it.\"\n",
            "\n",
            "And the long fight will continue.\n",
            "\n",
            "â€”\n",
            "\n",
            "The only way to keep a secret is to keep it.\n",
            "\n",
            "A few weeks ago, I had the opportunity to meet the husband of a former soldier who has been involved with the military for over 20 years.\n",
            "\n",
            "The man is the son of\n",
            "====================\n",
            "The secret of life is not necessarily to be found in the walls of the city, but in the walls of the city itself.\n",
            "\n",
            "\"The only way to preserve life is to go to the temple. I think the temple should be opened by the children of the temple. There should be a mass of men who would gather to go to the temple and to be paid their respects. They should be brought to the temple and pay their respects. They should be taken out of the temple and brought to the temple for their\n",
            "====================\n",
            "The secret of life is not to survive. It's to be where you are and to be where you are.\n",
            "\n",
            "But I do not believe we can survive in the world without this.\n",
            "\n",
            "The fact is that we have been through an enormous amount of suffering and we have been through a great deal of pain and we have been through a great deal of anguish and we have been through many painful experiences and we have been through many many painful deaths.\n",
            "\n",
            "I believe that we can survive without this. We can\n",
            "====================\n",
            "The secret of life is to work hard, to work hard, and to be happy. That's the way I look at it. I think of it as the good, the bad, the ugly, the ugly, the ugly, the ugly, the ugly. I think of it as the only way to be happy.\n",
            "\n",
            "The good news is that when you're working hard, you're going to get better. The bad news is that you're going to get worse. You're going to get worse. You\n",
            "====================\n",
            "The secret of life is to live within the pattern, and that is to live in the pattern.\n",
            "\n",
            "One way of knowing about what is going on in the universe is by looking at the stars. This is the way in which the cosmos is organized and the way in which the sun is set.\n",
            "\n",
            "The stars are the fundamental elements of the universe, which are connected with the planets, and they are the foundation of all life. The Sun and the planets are the foundation of all life.\n",
            "\n",
            "The planets\n",
            "====================\n"
          ]
        }
      ],
      "source": [
        "# sample model with stock phrase/prompt \"The secret of life is...\"\n",
        "# edit as you see fit\n",
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything returned above is the stock response of a pre-trained model (GPT-2), trained on a dataset gathered by OpenAI (outbound reddit links +3 karma)."
      ],
      "metadata": {
        "id": "vVUJphWCBJP4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BCf0PJxwl7g"
      },
      "source": [
        "## Part 1: Activity\n",
        "\n",
        "Think about what the capabilities and limits of the OpenAI dataset might be. And following those parameters, what the possibilities with the model are.\n",
        "\n",
        "How would you describe the \"voice\", \"personality\", or \"character\" that you are interacting with?\n",
        "\n",
        "Through a sequence of prompts, would it be possible to better understand what that voice and model are? Try to accomplish this in the code cells below. Add more as needed.\n",
        "\n",
        "At the end of your prompting the model, you will be asked to add a text cell describing your understanding of what you learned about the model, and what your prompts and completions were hoping to elicit from the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put your prompting code here\n",
        "# change the \"prefix\", that is your prompt.\n",
        "# change the temperature, nsamples, etc., as you see fit.\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )\n",
        "\n",
        "# add more code cells as needed"
      ],
      "metadata": {
        "id": "w3DKRoI-Cg5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Discussion\n",
        "\n",
        "Answer these in a text block below:\n",
        "\n",
        "1. How would you describe the \"voice\", \"personality\", or \"character\" of the pre-trained model? What perspective does it have? Does it have experience? Does it have values, needs, intelligence? These are the ways AI is discussed in popular media.\n",
        "\n",
        "2. What was your strategy with prompting the model? What were you thinking you could elicit through your queries?\n",
        "\n",
        "  a. were you satisfied with what you prompted? What might be a better approach?\n",
        "\n"
      ],
      "metadata": {
        "id": "H1gXCs58DLVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(EDIT THIS TEXT BOX and put your answers here. please delete this starter text I don't want to read it again)"
      ],
      "metadata": {
        "id": "NFFXyEEAD7bp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uatZmJ2Pwl7g"
      },
      "source": [
        "# 2: Fine-Tuning GPT-2\n",
        "\n",
        "Drawing on what you have concluded about the in-built perspective of GPT-2, now it is time to retrain the model (\"fine-tune\" it) to create something more specific that you want to interact with.\n",
        "\n",
        "Fine-tuning takes the general purpose \"generative pre-trained transformer\" (GPT) and narrows it to work within a smaller, specific set of training examples you have shown it.\n",
        "\n",
        "What kind of a more-specific model are you interested in interacting with? Something trained on your personal journals? The combined transcripts and textual footprint of a person you admire? The complete screenplays of a show you enjoyed or a movie/narrative world you loved but wish you could extend and inhabit?\n",
        "\n",
        "In this section you will provide this desired textual data and specialize the GPT-2 model through fine-tuning.\n",
        "\n",
        "You are asked to vary the parameters and duration of that fine-tuning in order to achieve a result that you think is ideal."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart and Reload\n",
        "Restart your runtime: **Runtime** -> **Restart session**.\n",
        "\n",
        "Run your imports, to reimport gpt, tensorflow, and things you need."
      ],
      "metadata": {
        "id": "YDI7oLuJHww7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import tensorflow as tf\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "YkIy3P5NIUvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify the model size, start your session, and load the model"
      ],
      "metadata": {
        "id": "dlNORU2zIyH-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeXshJM-Cuaf"
      },
      "outputs": [],
      "source": [
        "model_name = \"124M\"\n",
        "\n",
        "# start session and load model\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE"
      },
      "source": [
        "## Upload a text file\n",
        "Here you upload the text you provide to finetune (continue training) GPT-2. All of the text you want to you use should be saved in a single plain text (`.txt`) file.\n",
        "\n",
        "Simply drag and dropy our text file into the file browser at left.\n",
        "\n",
        "Once you have uploaded your file, update the file name in the cell below, then run it so our current python runtime knows what file to work with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "outputs": [],
      "source": [
        "file_name = \"script.txt\" # your file here. after you have uploaded, update the file name here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Run the finetuning\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every `save_every` steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them. If your input text is smaller, training might proceed more quickly.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRrCG_sqwl7g"
      },
      "outputs": [],
      "source": [
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name=model_name,\n",
        "              steps=200,\n",
        "              restore_from='fresh', # change to 'latest' to resume\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              learning_rate=1e-5,\n",
        "              sample_every=100,\n",
        "              save_every=200\n",
        "              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K"
      },
      "source": [
        "## Notes on finetuning\n",
        "\n",
        "Keep an eye on the loss, and how quickly it is dropping. A too-rapid drop in loss could be a sign of overfitting, and a learning rate (lr) that is too high. You could re-run this with a lower learning-rate.\n",
        "\n",
        "As the model is training, it saves checkpoints every few hundred iterations. A checkpoint contains the trained model weights and some other information about the training session.\n",
        "\n",
        "You can download the checkpoint folder to save your work, see below. Training checkpoints are saved to `checkpoint/run1` (or whatever you chose for the run name above).\n",
        "\n",
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Restoring Checkpoints"
      ],
      "metadata": {
        "id": "rVLTj2K05C9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compress your checkpoint and download it to your computer\n",
        "\n",
        "Run the code below. It will compress your checkpoint trained model waits and save a file called `checkpoint-run1.tar.gz`."
      ],
      "metadata": {
        "id": "DJ1CJdAtz_fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czvf \"/content/checkpoint-run1.tar.gz\" \"/content/checkpoint/run1\""
      ],
      "metadata": {
        "id": "MK3HMpyM0U8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find `checkpoint-run1.tar.gz` in the file browser at left and download it. (Right click the file then **Download**)"
      ],
      "metadata": {
        "id": "F0IUE7011CS1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX_2s026wl7g"
      },
      "source": [
        "## Optional: Finetune some more"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjqe-iwnwl7g"
      },
      "source": [
        "If you have already generated with gpt2, you need to reset the tf graph and gpt2 session. Otherwise, we create a new one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gunpu1_Mwl7g"
      },
      "outputs": [],
      "source": [
        "#model_name = \"124M\" # same model as selected above\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# check if sess exists (e.g. if we ran section 1 above)\n",
        "var_exists = 'sess' in locals() or 'sess' in globals()\n",
        "\n",
        "if not var_exists:\n",
        "    sess = gpt2.start_tf_sess()\n",
        "else:\n",
        "    sess = gpt2.reset_session(sess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DEzL_3Wwl7g"
      },
      "source": [
        "To fine-tune some more, run the following. Be sure to increase the number of steps (if it was `500` before, change to `1000` to train for 500 more. the number is cumulative)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrcWRTY6wl7g"
      },
      "outputs": [],
      "source": [
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name=model_name,\n",
        "              steps=400,\n",
        "              restore_from='latest', # change to 'latest' to resume\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              learning_rate=1e-5,\n",
        "              sample_every=100,\n",
        "              save_every=100\n",
        "              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Part 2: Activity - Generate Text From The Finetuned Model\n",
        "\n",
        "After you've trained the model (or loaded a pretrained model from checkpoint) you can now generate text. `generate` generates a single text from the loaded model.\n",
        "\n",
        "How is your fine-tuned model different from the stock GPT-2 (OpenAI pretrained)? In the section, feed it some completions that elicit the desired behavior you hoped for from the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unconditional Sampling\n",
        "\n",
        "Asking the model to generate a sample, without any prefix, is called \"unconditional generation\". It gives you a sense of what the model might do without any direction from you.\n",
        "\n",
        "Try this by running the cell below:"
      ],
      "metadata": {
        "id": "d02pFheLOQE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, run_name='run1') # no prefix, unconditional generation"
      ],
      "metadata": {
        "id": "HGRGBHuQOdFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "### Additional parameters for generation\n",
        "\n",
        "You can pass a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts). This is whate is now called `prompting`, with ChatGPT and related contemporary models.\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "* **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DKMc0fiej4N"
      },
      "outputs": [],
      "source": [
        "gpt2.generate(sess,\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              prefix=\"ham\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "outputs": [],
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-LRex8lfv1g"
      },
      "source": [
        "Download the file by hand in the browser at left."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: Discussion\n",
        "\n",
        "(answer the following in the text box below)\n",
        "\n",
        "1. Describe your strategy for prompting. What were you hoping to elicit? (1 paragraph)\n",
        "\n",
        "2. Describe your strategy for fine-tuning. What text did you use and where did you source it (include links)? How long did you let the fine-tuning run? How did you decide when to stop it? (1 paragraphs)\n",
        "\n",
        "3. What results did you get when generating from the fine-tuned model? How did they compare to results from the stock GPT-2 model? How did you adjust your prompts when interacting with the fine-tuned model? On balance, what do you think that fine-tuning might provide to work with a Large Language Model (LLM)?"
      ],
      "metadata": {
        "id": "dI5wwbZ8ZRYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(EDIT THIS TEXT BOX and put your answers here. please delete this starter text I don't want to read it again)"
      ],
      "metadata": {
        "id": "8y9OlciyaEBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit your notebook\n",
        "- (.ipynb) so I can run it again, including with a link to your training date\n",
        "- .pdf of your notebook."
      ],
      "metadata": {
        "id": "5ecahiFYaLH-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLN-TEHMwl7k"
      },
      "source": [
        "# Reference\n",
        "- Max's [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
        "- Original repo: [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple) by [Max Woolf](http://minimaxir.com).\n",
        "- Original [google colab](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) from Max."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "TensorFlow GPU 2.6 (py39)",
      "language": "python",
      "name": "tensorflow-gpu-2.6-py39"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}