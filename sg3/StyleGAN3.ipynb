{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roberttwomey/ml-art-code/blob/master/sg3/StyleGAN3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EMAR349 ML for the Arts - Twomey - [ml.roberttwomey.com](https://ml.roberttwomey.com)"
      ],
      "metadata": {
        "id": "Kim-wqRoPPMl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHzltei9Ue7I"
      },
      "source": [
        "#StyleGAN3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVF2P1OlT92b"
      },
      "source": [
        "This notebook explores **StyleGAN3** (aka Alias-Free GAN) released in [this repo](https://github.com/NVlabs/stylegan3) by NVidia in 2021. Adapted for ML for the Arts from [a Colab](https://colab.research.google.com/drive/1OkZZa5Yzt4scTwbwBORkki5-zNEAdXW1#scrollTo=IkBzNIQ9QsFB) produced by [crimeacs](https://twitter.com/EarthML1).\n",
        "____\n",
        "**[UPD 18.10.2021]** Added ThisSneakersDoesn'tExist model by [@stan_vossen](https://twitter.com/stan_vossen)  +  seems like [@l4rz](https://twitter.com/l4rz) killed the model for cosplay\n",
        "\n",
        "[UPD 17.10.2021] Added Music Video Generation (originally inspired by [this tweet](https://twitter.com/hexorcismos/status/1449032666574213125?s=20))\n",
        "\n",
        "[UPD 14.10.2021] Added Cosplay Faces trained by [@l4rz](https://twitter.com/l4rz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d99HsTjTQRzg"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade torch==1.9.1+cu111 torchvision==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install --upgrade torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install einops ninja\n",
        "#!pip install --upgrade https://download.pytorch.org/whl/nightly/cu111/torch-1.11.0.dev20211012%2Bcu111-cp37-cp37m-linux_x86_64.whl https://download.pytorch.org/whl/nightly/cu111/torchvision-0.12.0.dev20211012%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
        "!git clone https://github.com/NVlabs/stylegan3\n",
        "!pip install einops ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3VNGtzbvyKk"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('./stylegan3')\n",
        "\n",
        "import tensorflow\n",
        "import io\n",
        "import os, time\n",
        "import pickle\n",
        "import shutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkBzNIQ9QsFB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Generate an image\n",
        "#@markdown StyleGAN3 pre-trained models for config T (translation equiv.) and config R (translation and rotation equiv.)\n",
        "seed = 4011 #@param {type:\"slider\", min:0, max:9999, step:1}\n",
        "\n",
        "baselink ='https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/'\n",
        "model = \"stylegan3-r-metfaces-1024x1024.pkl\" #@param [\"sneakers\", \"stylegan2-cosplay-faces-512x512-px\", \"stylegan3-r-afhqv2-512x512.pkl\", \"stylegan3-r-ffhq-1024x1024.pkl\", \"stylegan3-r-ffhqu-1024x1024.pkl\",\"stylegan3-r-ffhqu-256x256.pkl\",\"stylegan3-r-metfaces-1024x1024.pkl\",\"stylegan3-r-metfacesu-1024x1024.pkl\",\"stylegan3-t-afhqv2-512x512.pkl\",\"stylegan3-t-ffhq-1024x1024.pkl\",\"stylegan3-t-ffhqu-1024x1024.pkl\",\"stylegan3-t-ffhqu-256x256.pkl\",\"stylegan3-t-metfaces-1024x1024.pkl\",\"stylegan3-t-metfacesu-1024x1024.pkl\"]\n",
        "\n",
        "if model == \"stylegan2-cosplay-faces-512x512-px\":\n",
        "    baselink = 'https://l4rz.net/'\n",
        "    model = 'cosplayface-snapshot-004000-18160-FID367.pkl'\n",
        "\n",
        "if model == 'sneakers':\n",
        "    if 'sneaksnap.pkl' not in os.listdir('/content/stylegan3'):\n",
        "        !gdown --id 1ReK9P4dkkClvpswdSuew35xCx2xjVsQa\n",
        "    baselink = '/content/stylegan3/'\n",
        "    model = 'sneaksnap.pkl'\n",
        "\n",
        "# Generate an image using pre-trained model\n",
        "!python stylegan3/gen_images.py --outdir=out --trunc=1 \\\n",
        " --seeds=$seed --network=$baselink$model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKI3HP6zx7Bz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "img = Image.open('/content/out/seed%04d.png' % seed);\n",
        "plt.imshow(img);\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open file browser at left and download still images from `/out` to to your local machine."
      ],
      "metadata": {
        "id": "-_c3rFkyQQGQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZREJuYfjbV3G"
      },
      "outputs": [],
      "source": [
        "#@title Generate an interpolation video\n",
        "%cd /content/stylegan3\n",
        "\n",
        "start_seed = 42 #@param {type:\"number\"}\n",
        "stop_seed = 669 #@param {type:\"number\"}\n",
        "n_cols =  1#@param {type:\"number\"}\n",
        "n_rows = 2 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown How many key frames to have?\n",
        "num_keyframes = 3 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown How many frames for interpolation?\n",
        "w_frames = 90 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Total length in frames is `num_keyframes`*`w_frames`\n",
        "\n",
        "assert stop_seed > start_seed, 'Stop_seed should be larger then start_seed'\n",
        "baselink ='https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/'\n",
        "model = \"stylegan3-r-metfaces-1024x1024.pkl\" #@param [\"sneakers\", \"stylegan2-cosplay-faces-512x512-px\", \"stylegan3-r-afhqv2-512x512.pkl\", \"stylegan3-r-ffhq-1024x1024.pkl\", \"stylegan3-r-ffhqu-1024x1024.pkl\",\"stylegan3-r-ffhqu-256x256.pkl\",\"stylegan3-r-metfaces-1024x1024.pkl\",\"stylegan3-r-metfacesu-1024x1024.pkl\",\"stylegan3-t-afhqv2-512x512.pkl\",\"stylegan3-t-ffhq-1024x1024.pkl\",\"stylegan3-t-ffhqu-1024x1024.pkl\",\"stylegan3-t-ffhqu-256x256.pkl\",\"stylegan3-t-metfaces-1024x1024.pkl\",\"stylegan3-t-metfacesu-1024x1024.pkl\"]\n",
        "\n",
        "if model == \"stylegan2-cosplay-faces-512x512-px\":\n",
        "    baselink = 'https://l4rz.net/'\n",
        "    model = 'cosplayface-snapshot-004000-18160-FID367.pkl'\n",
        "\n",
        "if model == 'sneakers':\n",
        "    if 'sneaksnap.pkl' not in os.listdir('/content/stylegan3'):\n",
        "        !gdown --id 1ReK9P4dkkClvpswdSuew35xCx2xjVsQa\n",
        "    baselink = '/content/stylegan3/'\n",
        "    model = 'sneaksnap.pkl'\n",
        "\n",
        "# Render a  grid of interpolations for seeds N through K.\n",
        "!python gen_video.py --output=lerp.mp4 --trunc=1 --seeds=$start_seed-$stop_seed --grid={n_rows}x{n_cols} \\\n",
        "    --network=$baselink$model --num-keyframes=$num_keyframes \\\n",
        "    --w-frames=$w_frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8K1ocKOryDfc"
      },
      "outputs": [],
      "source": [
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('lerp.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open file browser at left and download `lerp.mp4` to save it on your local machine. (Found in `/stylegan3` directory)"
      ],
      "metadata": {
        "id": "AFPnHNMnQHbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activities:\n",
        "- Try experimenting with different seeds. You can use the image generation code above to test out seeds.\n",
        "- Slow down the interpolation, generating a higher number of frames between keyframes.\n",
        "  - change `w_frames` in the code above.\n",
        "- Try a different number of keyframes.\n",
        "- Try a different model. See a list here: [awesome-pretrained-stylegan3](https://github.com/justinpinkney/awesome-pretrained-stylegan3)\n",
        "- Play with the truncation value (currently 1).\n",
        "  - Select **Show code** and modify the `--trunc=1` argument to `!python gen_video.py`\n",
        "- Make your own `gen_video.py` method below that pauses (holds) on each keyframe as opposed to smoothly interpolating through. (source is [here](https://github.com/NVlabs/stylegan3/blob/main/gen_video.py))\n"
      ],
      "metadata": {
        "id": "KYs-zahxR24g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zFWf0Wi1_4P_"
      },
      "outputs": [],
      "source": [
        "#@title # Optional: Generate ðŸŽµ music video\n",
        "#@markdown ##**Choose your settings**\n",
        "from IPython.display import clear_output\n",
        "%cd /content/stylegan3\n",
        "\n",
        "import requests\n",
        "import pickle\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import librosa\n",
        "from scipy.io import wavfile\n",
        "\n",
        "import time\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import ImageOps\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    basename = os.path.basename(url_or_path)\n",
        "    if os.path.exists(basename):\n",
        "        return basename\n",
        "    else:\n",
        "        !wget -c '{url_or_path}'\n",
        "        return basename\n",
        "\n",
        "baselink ='https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/'\n",
        "model = \"stylegan3-r-metfaces-1024x1024.pkl\" #@param [\"sneakers\", \"stylegan2-cosplay-faces-512x512-px\", \"stylegan3-r-afhqv2-512x512.pkl\", \"stylegan3-r-ffhq-1024x1024.pkl\", \"stylegan3-r-ffhqu-1024x1024.pkl\",\"stylegan3-r-ffhqu-256x256.pkl\",\"stylegan3-r-metfaces-1024x1024.pkl\",\"stylegan3-r-metfacesu-1024x1024.pkl\",\"stylegan3-t-afhqv2-512x512.pkl\",\"stylegan3-t-ffhq-1024x1024.pkl\",\"stylegan3-t-ffhqu-1024x1024.pkl\",\"stylegan3-t-ffhqu-256x256.pkl\",\"stylegan3-t-metfaces-1024x1024.pkl\",\"stylegan3-t-metfacesu-1024x1024.pkl\"]\n",
        "\n",
        "if model == \"stylegan2-cosplay-faces-512x512-px\":\n",
        "    baselink = 'https://l4rz.net/'\n",
        "    model = 'cosplayface-snapshot-004000-18160-FID367.pkl'\n",
        "\n",
        "network_url = baselink + model\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "if model == 'sneakers':\n",
        "    if 'sneaksnap.pkl' not in os.listdir('/content/stylegan3'):\n",
        "        !gdown --id 1ReK9P4dkkClvpswdSuew35xCx2xjVsQa\n",
        "    network_url = '/content/stylegan3/sneaksnap.pkl'\n",
        "\n",
        "with open(fetch_model(network_url), 'rb') as fp:\n",
        "  G = pickle.load(fp)['G_ema'].to(device)\n",
        "\n",
        "seed =  42#@param {type:\"number\"}\n",
        "\n",
        "#@markdown How variable should the video be? (lower values - less variable)\n",
        "#if you are reading that - you are smart enough to map frequencies to psi as well\n",
        "truncation_psi = 0.5 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown How *strongly* should the image change?\n",
        "effect_strength =  1#@param {type:\"number\"}\n",
        "\n",
        "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
        "w_stds = G.mapping(zs, None).std(0)\n",
        "\n",
        "#@markdown Link to MP3 audio file (you can also extact music from a Youtube link)\n",
        "audio_link = 'https://cdn.pixabay.com/download/audio/2021/03/26/audio_dd57ac8732.mp3?filename=east-2-west-3513.mp3' #@param {type:\"string\"}\n",
        "if 'youtu.be' not in audio_link:\n",
        "    !wget {audio_link} -O audio.mp3\n",
        "else:\n",
        "    !youtube-dl --extract-audio --audio-format mp3 https://youtu.be/0OkiUUU3Odw -o music_temp.mp3\n",
        "    !ffmpeg -i music_temp.mp3 -af silenceremove=1:0:-50dB audio.mp3\n",
        "\n",
        "#@markdown Cut audio to N seconds\n",
        "cut_start =  15#@param {type:\"number\"}\n",
        "cut_end =  30#@param {type:\"number\"}\n",
        "\n",
        "cut_len = cut_end-cut_start\n",
        "\n",
        "#@markdown How many frames to use for interpolation?\n",
        "interp_frames =  5#@param {type:\"number\"}\n",
        "\n",
        "#@markdown Which frequencies to use?\n",
        "freqs = 'all' #@param ['low', 'high', 'all']\n",
        "\n",
        "arr, fr = librosa.load('audio.mp3')\n",
        "arr = arr[int(fr*cut_start):int(fr*cut_end)]\n",
        "\n",
        "wavfile.write('audio.wav', fr, arr)\n",
        "\n",
        "# stft = torch.stft(torch.tensor(arr),\n",
        "#            G.mapping.z_dim*2-1,\n",
        "#            hop_length=G.mapping.z_dim//4,\n",
        "#            center=False,\n",
        "#            pad_mode='reflect',\n",
        "#            normalized=True,\n",
        "#            onesided=True,\n",
        "#            return_complex=True)\n",
        "\n",
        "stft=librosa.feature.melspectrogram(y=arr,\n",
        "                               sr=fr,\n",
        "                               n_fft=2048,\n",
        "                               hop_length=G.mapping.z_dim*4,\n",
        "                               n_mels=G.mapping.z_dim)\n",
        "\n",
        "stft = torch.log(torch.tensor(stft).abs())\n",
        "\n",
        "if freqs == 'low':\n",
        "    stft[stft.size(0)//2:, :] *= 10\n",
        "\n",
        "if freqs == 'high':\n",
        "    stft[:stft.size(0)//2, :] *= 10\n",
        "\n",
        "clear_output()\n",
        "\n",
        "#FRAMES\n",
        "import time\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "zq = []\n",
        "with torch.no_grad():\n",
        "    timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "    # rand_z = torch.randn(stft.size(-1), G.mapping.z_dim).to(device)\n",
        "    # q = (G.mapping(rand_z, None, truncation_psi=truncation_psi))\n",
        "\n",
        "    for i in range(stft.size(-1)):\n",
        "        frame = stft[:,i].T.to(device)\n",
        "        z = torch.mean(G.mapping(frame.unsqueeze(0), None, truncation_psi=truncation_psi), dim=0)\n",
        "        zq.append(z.unsqueeze(0)*effect_strength)\n",
        "\n",
        "    count = 0\n",
        "    for k in tqdm(range(len(zq)-1)):\n",
        "        i_val = torch.linspace(0,1,interp_frames).to(device)\n",
        "        for interpolation in tqdm(i_val, leave=False):\n",
        "            interp = torch.lerp(zq[k], zq[k+1], interpolation)\n",
        "            images = G.synthesis(interp)\n",
        "            images = ((images + 1)/2).clamp(0,1)\n",
        "            pil_image = TF.to_pil_image(images[0].cpu())\n",
        "            if model == 'sneakers':\n",
        "                pil_image = ImageOps.invert(pil_image)\n",
        "            os.makedirs(f'samples/{timestring}', exist_ok=True)\n",
        "            pil_image.save(f'samples/{timestring}/{count:04}.png')\n",
        "            count+=1\n",
        "\n",
        "\n",
        "#VIDEO\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "fps = count/cut_len\n",
        "\n",
        "frames = []\n",
        "# tqdm.write('Generating video...')\n",
        "for i in sorted(os.listdir(f'samples/{timestring}')): #\n",
        "    frames.append(Image.open(f\"samples/{timestring}/{i}\"))\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', 'video.mp4'], stdin=PIPE)\n",
        "for im in tqdm(frames):\n",
        "    im.save(p.stdin, 'PNG')\n",
        "p.stdin.close()\n",
        "p.wait()\n",
        "\n",
        "!ffmpeg -y -i video.mp4 -i audio.wav -map 0 -map 1:a -c:v copy -shortest video_audio.mp4\n",
        "\n",
        "clear_output()\n",
        "# mp4 = open('video.mp4','rb').read()\n",
        "mp4 = open('video_audio.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "display.HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)\n",
        "\n",
        "#@markdown P.S.: *If it crushed - look for `video-audio.mp4` in `stylegan3` folder*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9Vsm7h-yi25b"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# # work in progress\n",
        "# # make visualizer\n",
        "# # stop looping, start parallelizing\n",
        "# # Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "# !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "# %cd Real-ESRGAN\n",
        "# # Set up the environment\n",
        "# !pip install --upgrade basicsr\n",
        "# # !pip install facexlib\n",
        "# # !pip install gfpgan\n",
        "# # !pip install -r requirements.txt\n",
        "# # !python setup.py develop\n",
        "# # # Download the pre-trained model\n",
        "# # !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}