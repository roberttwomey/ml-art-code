{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roberttwomey/ml-art-code/blob/master/vgg_walkthrough/vgg19_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoHIGPd7WScz"
      },
      "source": [
        "EMAR349 ML for the Arts - Twomey - Spring 2024 - [ml.roberttwomey.com](http://ml.roberttwomey.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27a60jG0WSc0"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvqNFQPvWSc0"
      },
      "source": [
        "# Keras VGG19 Example for Image Classification\n",
        "Import the keras modules and helper functions, as well as matplotlib.\n",
        "\n",
        "Load the VGG19 model (from [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556), ICLR 2015)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uQEF1sBWSc0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "model = VGG19()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeDCPSzZWSc0"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NTMbaoVWSc0"
      },
      "outputs": [],
      "source": [
        "# uncomment the following to plot a picture of the model\n",
        "\n",
        "# from tensorflow.keras.utils import plot_model\n",
        "# plot_model(model, to_file='vgg.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VojW91oBWSc0"
      },
      "source": [
        "the images we are going to analyze:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/roberttwomey/ml-art-code/master/vgg_walkthrough/img/bus.jpg\n",
        "!wget https://raw.githubusercontent.com/roberttwomey/ml-art-code/master/vgg_walkthrough/img/elephant.jpg"
      ],
      "metadata": {
        "id": "BF0zQ8AJWirW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXh7GJpgWSc0"
      },
      "outputs": [],
      "source": [
        "fileList=['bus.jpg', 'elephant.jpg']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classify the Image"
      ],
      "metadata": {
        "id": "cUlP8T8Ai3M2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmZHs4dnWSc0"
      },
      "outputs": [],
      "source": [
        "for image_path in fileList:\n",
        "\n",
        "    # load an image from file\n",
        "    image = load_img(image_path, target_size=(224, 224))\n",
        "\n",
        "    im2show = image\n",
        "\n",
        "    # convert the image pixels to a numpy array\n",
        "    image = img_to_array(image)\n",
        "\n",
        "    # reshape data for the model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\n",
        "    # prepare the image for the VGG model\n",
        "    image = preprocess_input(image)\n",
        "\n",
        "    # predict the probability across all output classes\n",
        "    yhat = model.predict(image)\n",
        "\n",
        "    # convert the probabilities to class labels\n",
        "    label = decode_predictions(yhat)\n",
        "    # retrieve the most likely result, e.g. highest probability\n",
        "    # label = label[0][0]\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(im2show)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"*******\")\n",
        "    # print matched labels\n",
        "    for label in label[0]:\n",
        "        # print the classification\n",
        "        print('%s (%.2f%%)' % (label[1], label[2]*100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFoqiA2OWSc1"
      },
      "source": [
        "# A Brief Tour of Layer Activations\n",
        "We will step down through the layers and view a few activation maps. Define some helper functions to read and display the activations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Model\n",
        "\n",
        "layer_names = ['input_1',\n",
        "                'block1_conv1', 'block1_conv2', 'block1_pool',\n",
        "                'block2_conv1', 'block2_conv2', 'block2_pool',\n",
        "                'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_conv4', 'block3_pool',\n",
        "                'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_conv4', 'block4_pool',\n",
        "                'block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_conv4', 'block5_pool',\n",
        "                'flatten', 'fc1', 'fc2', 'predictions' ]\n",
        "\n",
        "outputs = [model.get_layer(layer).output for layer in layer_names]\n",
        "\n",
        "this_model = Model(inputs=model.input, outputs=outputs)\n",
        "\n",
        "activations = this_model.predict(image)"
      ],
      "metadata": {
        "id": "vIkyw8ypWM9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step through the layers of the network\n",
        "\n",
        "Run each of the below cells to display the feature maps (\"activations\") of each layer of the model."
      ],
      "metadata": {
        "id": "KN_9ckUii7T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(activations[0].shape)\n",
        "plt.imshow(activations[0][0][:,:,0])"
      ],
      "metadata": {
        "id": "_AJh5m7IXRjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHK8FElFWSc1"
      },
      "outputs": [],
      "source": [
        "len(activations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KKVn2nMWSc1"
      },
      "source": [
        "Step through layers and view the activation maps. First, let's explore layer 0.\n",
        "\n",
        "__Layer 0__: 224 x 224 pixels, 3 feature maps (channels).\n",
        "\n",
        "This is the input image with three channels (RGB)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activations[0].shape"
      ],
      "metadata": {
        "id": "CR6Kbmsjf4_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "show one of those images (R, G, or B). he last digit in this array `[0,:,:,0]` selects one of those 3 channels. (Currently __0__. Can be anything from __0-2__)"
      ],
      "metadata": {
        "id": "ZtUlK3RygD6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(activations[0][0][:,:,0])"
      ],
      "metadata": {
        "id": "NXqRrqECgBdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "__Layer 1__: 224 x 224 pixels, 64 feature maps (channels)"
      ],
      "metadata": {
        "id": "LXYpEP22f8Yv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06UZwIbYWSc1"
      },
      "outputs": [],
      "source": [
        "activations[1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdwP2kp9WSc1"
      },
      "source": [
        "Select an individual feature map. The last digit in this array `[0,:,:,0]` selects one of those 64 channels. (Currently __0__. Can be anything from __0__-__63__) These are the first \"features\" extracted from the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5phNG_6zWSc1"
      },
      "outputs": [],
      "source": [
        "activations[1][0,:,:,0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRi0jWMQWSc1"
      },
      "outputs": [],
      "source": [
        "plt.imshow(activations[0][0,:,:,2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3izXGWlWSc1"
      },
      "source": [
        "__Layer 2__: 224 x 224 pixels, 64 channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GR3ayVwWSc1"
      },
      "outputs": [],
      "source": [
        "activations[2].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVtb6ZIwWSc1"
      },
      "outputs": [],
      "source": [
        "activations[2][0,:,:,23].shape # 23 selects the 23rd feature map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r84aYyMWSc1"
      },
      "outputs": [],
      "source": [
        "plt.imshow(activations[2][0,:,:,23])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Lz6S_aWSc1"
      },
      "source": [
        "__Layer 3__: 112 x 112 pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVaftDAJWSc1"
      },
      "outputs": [],
      "source": [
        "activations[3].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTjge7o9WSc1"
      },
      "outputs": [],
      "source": [
        "activations[3][0,:,:,47].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13ZS2PEQWSc1"
      },
      "outputs": [],
      "source": [
        "plt.imshow(activations[3][0,:,:,47])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVfL12czWSc2"
      },
      "source": [
        "__Layer 6__: 56 x 56 pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qESIsvwYWSc2"
      },
      "outputs": [],
      "source": [
        "activations[6].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-vqGKWRWSc2"
      },
      "outputs": [],
      "source": [
        "activations[6][0,:,:,17].shape # 17th out of 128 feature maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg4QrLiAWSc2"
      },
      "outputs": [],
      "source": [
        "plt.imshow(activations[6][0,:,:,17])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMcQ4vTdWSc2"
      },
      "source": [
        "__Layer 11__: 28 x 28 pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoxd0gSyWSc2"
      },
      "outputs": [],
      "source": [
        "print(activations[11].shape)\n",
        "print(activations[11][0,:,:,10].shape)\n",
        "plt.imshow(activations[11][0,:,:,10]) # 10 out of 256 feature maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU2_a875WSc2"
      },
      "source": [
        "__Layer 16__: 14 x 14 pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYfp3AIbWSc2"
      },
      "outputs": [],
      "source": [
        "print(activations[16].shape)\n",
        "print(activations[16][0,:,:,1].shape)\n",
        "plt.imshow(activations[16][0,:,:,1]) # 1 out of 512 feature maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUUSoLu1WSc2"
      },
      "source": [
        "__Layer 25__: The last layer, 1 x 1000 wth softmax (categories).\n",
        "\n",
        "First we can just print the values of the layer. This is hard to parse as a human reader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrqRSqzdWSc2"
      },
      "outputs": [],
      "source": [
        "print(activations[25])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln8_C3BYWSc2"
      },
      "source": [
        "Reshaped to be 25 x 40 instead of a single long vector. The highest value (yellow spot) corresponds to the most likely predicted category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHnJ30-jWSc2"
      },
      "outputs": [],
      "source": [
        "mtx=activations[25][0]\n",
        "mtx.shape\n",
        "mtx2=mtx.reshape(25,40)\n",
        "fig=plt.imshow(mtx2)\n",
        "plt.colorbar(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPDEUseJWSc2"
      },
      "outputs": [],
      "source": [
        "print(\"*******\")\n",
        "label = decode_predictions(activations[25])\n",
        "# print matched labels\n",
        "for label in label[0]:\n",
        "    # print the classification\n",
        "    print('%s (%.2f%%)' % (label[1], label[2]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyfQSCnyWSc_"
      },
      "source": [
        "# Activities\n",
        "- Rerun some of the above cells, but to select a different channel/feature map. (change the last number in the array).\n",
        "  - Can you discover any legible features?\n",
        "- Upload your own image and run the image classification.\n",
        "  - To add your own image, upload the file in the browser at right and add it to the end of the `filelist` array above. Re-run the classification step.\n",
        "  - is the prediction correct?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (tf-gpu-1.14)",
      "language": "python",
      "name": "tf-gpu-1.14"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}